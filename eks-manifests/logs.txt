* 
* ==> Audit <==
* |---------|----------------------------|----------|-------|---------|---------------------|---------------------|
| Command |            Args            | Profile  | User  | Version |     Start Time      |      End Time       |
|---------|----------------------------|----------|-------|---------|---------------------|---------------------|
| start   |                            | minikube | shubz | v1.31.2 | 14 Oct 23 00:46 IST |                     |
| delete  |                            | minikube | shubz | v1.31.2 | 14 Oct 23 01:02 IST | 14 Oct 23 01:02 IST |
| start   |                            | minikube | shubz | v1.31.2 | 14 Oct 23 01:52 IST |                     |
| start   |                            | minikube | shubz | v1.31.2 | 14 Oct 23 02:05 IST | 14 Oct 23 02:07 IST |
| start   | --driver=docker            | minikube | shubz | v1.32.0 | 24 Jan 24 20:20 IST |                     |
| start   | --driver=docker            | minikube | shubz | v1.32.0 | 24 Jan 24 20:21 IST |                     |
| start   |                            | minikube | shubz | v1.32.0 | 24 Jan 24 20:22 IST | 24 Jan 24 20:27 IST |
| start   |                            | minikube | shubz | v1.32.0 | 24 Jan 24 20:44 IST | 24 Jan 24 20:47 IST |
| service | two-tier-app-service --url | minikube | shubz | v1.32.0 | 24 Jan 24 21:03 IST | 24 Jan 24 21:03 IST |
| start   |                            | minikube | shubz | v1.32.0 | 27 Jan 24 01:48 IST | 27 Jan 24 01:50 IST |
| service | two-tier-app-service --url | minikube | shubz | v1.32.0 | 27 Jan 24 01:55 IST | 27 Jan 24 01:56 IST |
| stop    |                            | minikube | shubz | v1.32.0 | 27 Jan 24 01:57 IST | 27 Jan 24 01:57 IST |
| start   |                            | minikube | shubz | v1.32.0 | 28 Jan 24 15:06 IST | 28 Jan 24 15:08 IST |
| service | mysql --url                | minikube | shubz | v1.32.0 | 28 Jan 24 15:16 IST | 28 Jan 24 15:16 IST |
| service | two-tier-app-service --url | minikube | shubz | v1.32.0 | 28 Jan 24 15:17 IST |                     |
|---------|----------------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/28 15:06:04
Running on machine: shubz
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0128 15:06:04.125006   23984 out.go:296] Setting OutFile to fd 1 ...
I0128 15:06:04.125303   23984 out.go:348] isatty.IsTerminal(1) = true
I0128 15:06:04.125312   23984 out.go:309] Setting ErrFile to fd 2...
I0128 15:06:04.125326   23984 out.go:348] isatty.IsTerminal(2) = true
I0128 15:06:04.125823   23984 root.go:338] Updating PATH: /home/shubz/.minikube/bin
I0128 15:06:04.156972   23984 out.go:303] Setting JSON to false
I0128 15:06:04.227677   23984 start.go:128] hostinfo: {"hostname":"shubz","uptime":53760,"bootTime":1706380804,"procs":299,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-91-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"f227a317-db88-423a-8186-03faaf250270"}
I0128 15:06:04.227944   23984 start.go:138] virtualization: kvm host
I0128 15:06:04.336004   23984 out.go:177] 😄  minikube v1.32.0 on Ubuntu 22.04
I0128 15:06:04.458476   23984 notify.go:220] Checking for updates...
I0128 15:06:04.460443   23984 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 15:06:04.535951   23984 out.go:177] 🆕  Kubernetes 1.28.3 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.28.3
I0128 15:06:04.630996   23984 driver.go:378] Setting default libvirt URI to qemu:///system
I0128 15:06:05.203265   23984 docker.go:122] docker version: linux-24.0.5:
I0128 15:06:05.203528   23984 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 15:06:05.339184   23984 info.go:266] docker info: {ID:97e1f478-da12-4545-8716-3201653fa8a1 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:34 SystemTime:2024-01-28 15:06:05.240598663 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-91-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6081265664 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shubz Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0128 15:06:05.339403   23984 docker.go:295] overlay module found
I0128 15:06:05.434851   23984 out.go:177] ✨  Using the docker driver based on existing profile
I0128 15:06:05.545696   23984 start.go:298] selected driver: docker
I0128 15:06:05.545731   23984 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.94.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shubz:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:0s GPUs:}
I0128 15:06:05.546099   23984 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0128 15:06:05.546443   23984 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 15:06:05.579395   23984 info.go:266] docker info: {ID:97e1f478-da12-4545-8716-3201653fa8a1 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:34 SystemTime:2024-01-28 15:06:05.567805955 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-91-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6081265664 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shubz Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0128 15:06:05.608914   23984 cni.go:84] Creating CNI manager for ""
I0128 15:06:05.608945   23984 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 15:06:05.608961   23984 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.94.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shubz:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:0s GPUs:}
I0128 15:06:05.712633   23984 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0128 15:06:05.779280   23984 cache.go:121] Beginning downloading kic base image for docker with docker
I0128 15:06:05.846010   23984 out.go:177] 🚜  Pulling base image ...
I0128 15:06:05.979126   23984 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0128 15:06:05.992890   23984 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0128 15:06:05.992986   23984 preload.go:148] Found local preload: /home/shubz/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0128 15:06:05.992995   23984 cache.go:56] Caching tarball of preloaded images
I0128 15:06:05.993192   23984 preload.go:174] Found /home/shubz/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0128 15:06:05.993206   23984 cache.go:59] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0128 15:06:05.993345   23984 profile.go:148] Saving config to /home/shubz/.minikube/profiles/minikube/config.json ...
I0128 15:06:06.048217   23984 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0128 15:06:06.048234   23984 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0128 15:06:06.048252   23984 cache.go:194] Successfully downloaded all kic artifacts
I0128 15:06:06.048296   23984 start.go:365] acquiring machines lock for minikube: {Name:mk75207d62ec0a7381c1b23ab035c22e4cf97392 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0128 15:06:06.048439   23984 start.go:369] acquired machines lock for "minikube" in 121.448µs
I0128 15:06:06.048460   23984 start.go:96] Skipping create...Using existing machine configuration
I0128 15:06:06.048470   23984 fix.go:54] fixHost starting: 
I0128 15:06:06.048826   23984 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 15:06:06.085727   23984 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0128 15:06:06.085751   23984 fix.go:128] unexpected machine state, will restart: <nil>
I0128 15:06:06.253596   23984 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0128 15:06:06.479574   23984 cli_runner.go:164] Run: docker start minikube
I0128 15:06:12.181913   23984 cli_runner.go:217] Completed: docker start minikube: (5.70225206s)
I0128 15:06:12.182198   23984 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 15:06:12.211876   23984 kic.go:430] container "minikube" state is running.
I0128 15:06:12.212328   23984 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 15:06:12.256380   23984 profile.go:148] Saving config to /home/shubz/.minikube/profiles/minikube/config.json ...
I0128 15:06:12.257036   23984 machine.go:88] provisioning docker machine ...
I0128 15:06:12.257148   23984 ubuntu.go:169] provisioning hostname "minikube"
I0128 15:06:12.257294   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:12.283447   23984 main.go:141] libmachine: Using SSH client type: native
I0128 15:06:12.294447   23984 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 15:06:12.294491   23984 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0128 15:06:12.295746   23984 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:48714->127.0.0.1:32772: read: connection reset by peer
I0128 15:06:15.296649   23984 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:58838->127.0.0.1:32772: read: connection reset by peer
I0128 15:06:18.318125   23984 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:58842->127.0.0.1:32772: read: connection reset by peer
I0128 15:06:24.794349   23984 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 15:06:24.794570   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:24.819545   23984 main.go:141] libmachine: Using SSH client type: native
I0128 15:06:24.820196   23984 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 15:06:24.820218   23984 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0128 15:06:25.068610   23984 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 15:06:25.068691   23984 ubuntu.go:175] set auth options {CertDir:/home/shubz/.minikube CaCertPath:/home/shubz/.minikube/certs/ca.pem CaPrivateKeyPath:/home/shubz/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/shubz/.minikube/machines/server.pem ServerKeyPath:/home/shubz/.minikube/machines/server-key.pem ClientKeyPath:/home/shubz/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/shubz/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/shubz/.minikube}
I0128 15:06:25.068770   23984 ubuntu.go:177] setting up certificates
I0128 15:06:25.068796   23984 provision.go:83] configureAuth start
I0128 15:06:25.068992   23984 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 15:06:25.094066   23984 provision.go:138] copyHostCerts
I0128 15:06:25.205512   23984 exec_runner.go:144] found /home/shubz/.minikube/ca.pem, removing ...
I0128 15:06:25.205581   23984 exec_runner.go:203] rm: /home/shubz/.minikube/ca.pem
I0128 15:06:25.205822   23984 exec_runner.go:151] cp: /home/shubz/.minikube/certs/ca.pem --> /home/shubz/.minikube/ca.pem (1074 bytes)
I0128 15:06:25.257454   23984 exec_runner.go:144] found /home/shubz/.minikube/cert.pem, removing ...
I0128 15:06:25.257485   23984 exec_runner.go:203] rm: /home/shubz/.minikube/cert.pem
I0128 15:06:25.257620   23984 exec_runner.go:151] cp: /home/shubz/.minikube/certs/cert.pem --> /home/shubz/.minikube/cert.pem (1119 bytes)
I0128 15:06:25.304109   23984 exec_runner.go:144] found /home/shubz/.minikube/key.pem, removing ...
I0128 15:06:25.304144   23984 exec_runner.go:203] rm: /home/shubz/.minikube/key.pem
I0128 15:06:25.304283   23984 exec_runner.go:151] cp: /home/shubz/.minikube/certs/key.pem --> /home/shubz/.minikube/key.pem (1675 bytes)
I0128 15:06:25.317020   23984 provision.go:112] generating server cert: /home/shubz/.minikube/machines/server.pem ca-key=/home/shubz/.minikube/certs/ca.pem private-key=/home/shubz/.minikube/certs/ca-key.pem org=shubz.minikube san=[192.168.94.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0128 15:06:25.767684   23984 provision.go:172] copyRemoteCerts
I0128 15:06:25.767783   23984 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0128 15:06:25.767840   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:25.792294   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:06:25.901036   23984 ssh_runner.go:362] scp /home/shubz/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0128 15:06:26.475366   23984 ssh_runner.go:362] scp /home/shubz/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0128 15:06:26.630241   23984 ssh_runner.go:362] scp /home/shubz/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0128 15:06:26.734538   23984 provision.go:86] duration metric: configureAuth took 1.665703565s
I0128 15:06:26.734598   23984 ubuntu.go:193] setting minikube options for container-runtime
I0128 15:06:26.735125   23984 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 15:06:26.735275   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:26.763761   23984 main.go:141] libmachine: Using SSH client type: native
I0128 15:06:26.764361   23984 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 15:06:26.764373   23984 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0128 15:06:27.029699   23984 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0128 15:06:27.029726   23984 ubuntu.go:71] root file system type: overlay
I0128 15:06:27.030086   23984 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0128 15:06:27.030260   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:27.055980   23984 main.go:141] libmachine: Using SSH client type: native
I0128 15:06:27.056908   23984 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 15:06:27.057111   23984 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0128 15:06:27.406957   23984 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0128 15:06:27.407051   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:27.429958   23984 main.go:141] libmachine: Using SSH client type: native
I0128 15:06:27.430557   23984 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 15:06:27.430590   23984 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0128 15:06:27.792961   23984 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 15:06:27.792996   23984 machine.go:91] provisioned docker machine in 15.535929596s
I0128 15:06:27.793017   23984 start.go:300] post-start starting for "minikube" (driver="docker")
I0128 15:06:27.793040   23984 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0128 15:06:27.793169   23984 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0128 15:06:27.793256   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:27.823618   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:06:28.772870   23984 ssh_runner.go:195] Run: cat /etc/os-release
I0128 15:06:28.779970   23984 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0128 15:06:28.780050   23984 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0128 15:06:28.780084   23984 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0128 15:06:28.780100   23984 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0128 15:06:28.780135   23984 filesync.go:126] Scanning /home/shubz/.minikube/addons for local assets ...
I0128 15:06:28.811286   23984 filesync.go:126] Scanning /home/shubz/.minikube/files for local assets ...
I0128 15:06:28.825661   23984 start.go:303] post-start completed in 1.032622985s
I0128 15:06:28.825862   23984 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0128 15:06:28.826005   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:28.895860   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:06:29.158474   23984 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0128 15:06:29.289175   23984 fix.go:56] fixHost completed within 23.240701326s
I0128 15:06:29.289201   23984 start.go:83] releasing machines lock for "minikube", held for 23.240747969s
I0128 15:06:29.289282   23984 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 15:06:29.311240   23984 ssh_runner.go:195] Run: cat /version.json
I0128 15:06:29.311310   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:29.311384   23984 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0128 15:06:29.311462   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:06:29.344771   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:06:29.428823   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
W0128 15:06:29.477453   23984 out.go:239] ❗  Image was not built for the current minikube version. To resolve this you can delete and recreate your minikube cluster using the latest images. Expected minikube version: v1.31.0 -> Actual minikube version: v1.32.0
I0128 15:06:29.520215   23984 ssh_runner.go:195] Run: systemctl --version
I0128 15:06:32.176098   23984 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.864642597s)
I0128 15:06:32.176280   23984 ssh_runner.go:235] Completed: systemctl --version: (2.655997569s)
I0128 15:06:32.176514   23984 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0128 15:06:32.203336   23984 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0128 15:06:32.321309   23984 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0128 15:06:32.321441   23984 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0128 15:06:32.337364   23984 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0128 15:06:32.337394   23984 start.go:472] detecting cgroup driver to use...
I0128 15:06:32.337432   23984 detect.go:199] detected "systemd" cgroup driver on host os
I0128 15:06:32.337613   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 15:06:32.393353   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0128 15:06:32.456603   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0128 15:06:32.475956   23984 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0128 15:06:32.476105   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0128 15:06:32.491041   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 15:06:32.506363   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0128 15:06:32.524344   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 15:06:32.540577   23984 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0128 15:06:32.555760   23984 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0128 15:06:32.570022   23984 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0128 15:06:32.620150   23984 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0128 15:06:32.631779   23984 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:06:32.875986   23984 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0128 15:06:33.129480   23984 start.go:472] detecting cgroup driver to use...
I0128 15:06:33.129520   23984 detect.go:199] detected "systemd" cgroup driver on host os
I0128 15:06:33.129578   23984 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0128 15:06:33.147837   23984 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0128 15:06:33.147940   23984 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0128 15:06:33.249373   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 15:06:33.279668   23984 ssh_runner.go:195] Run: which cri-dockerd
I0128 15:06:33.285712   23984 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0128 15:06:33.299744   23984 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0128 15:06:33.329875   23984 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0128 15:06:33.520219   23984 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0128 15:06:33.649823   23984 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0128 15:06:33.649976   23984 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0128 15:06:33.758359   23984 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:06:33.890475   23984 ssh_runner.go:195] Run: sudo systemctl restart docker
I0128 15:06:39.875677   23984 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.98517002s)
I0128 15:06:39.875751   23984 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 15:06:40.031513   23984 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0128 15:06:40.164045   23984 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 15:06:40.298048   23984 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:06:40.424659   23984 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0128 15:06:40.447933   23984 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:06:40.578461   23984 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0128 15:06:42.617709   23984 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (2.039210425s)
I0128 15:06:42.617736   23984 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0128 15:06:42.617864   23984 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0128 15:06:42.622474   23984 start.go:540] Will wait 60s for crictl version
I0128 15:06:42.622577   23984 ssh_runner.go:195] Run: which crictl
I0128 15:06:42.626824   23984 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0128 15:06:44.149823   23984 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.522965512s)
I0128 15:06:44.182147   23984 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0128 15:06:44.182278   23984 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 15:06:45.220992   23984 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (1.03868259s)
I0128 15:06:45.221087   23984 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 15:06:45.313604   23984 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0128 15:06:45.314031   23984 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0128 15:06:45.429939   23984 ssh_runner.go:195] Run: grep 192.168.94.1	host.minikube.internal$ /etc/hosts
I0128 15:06:45.436012   23984 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.94.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 15:06:45.621745   23984 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0128 15:06:45.621879   23984 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 15:06:45.667960   23984 docker.go:671] Got preloaded images: -- stdout --
mysql:latest
trainwithshubham/flaskapp:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 15:06:45.667985   23984 docker.go:601] Images already preloaded, skipping extraction
I0128 15:06:45.668071   23984 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 15:06:45.710671   23984 docker.go:671] Got preloaded images: -- stdout --
mysql:latest
trainwithshubham/flaskapp:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 15:06:45.710692   23984 cache_images.go:84] Images are preloaded, skipping loading
I0128 15:06:45.710767   23984 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0128 15:06:47.360046   23984 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.649243087s)
I0128 15:06:47.360111   23984 cni.go:84] Creating CNI manager for ""
I0128 15:06:47.360136   23984 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 15:06:47.360164   23984 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0128 15:06:47.360210   23984 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.94.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.94.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.94.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0128 15:06:47.360440   23984 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.94.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.94.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.94.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0128 15:06:47.360581   23984 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.94.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0128 15:06:47.360689   23984 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0128 15:06:47.390949   23984 binaries.go:44] Found k8s binaries, skipping transfer
I0128 15:06:47.391034   23984 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0128 15:06:47.416409   23984 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0128 15:06:47.496263   23984 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0128 15:06:47.525132   23984 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0128 15:06:47.582351   23984 ssh_runner.go:195] Run: grep 192.168.94.2	control-plane.minikube.internal$ /etc/hosts
I0128 15:06:47.587467   23984 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.94.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 15:06:47.622877   23984 certs.go:56] Setting up /home/shubz/.minikube/profiles/minikube for IP: 192.168.94.2
I0128 15:06:47.622911   23984 certs.go:190] acquiring lock for shared ca certs: {Name:mk6feec1b076e953b3e5af28b7b20a2bc383b8cf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:06:47.638581   23984 certs.go:199] skipping minikubeCA CA generation: /home/shubz/.minikube/ca.key
I0128 15:06:47.638957   23984 certs.go:199] skipping proxyClientCA CA generation: /home/shubz/.minikube/proxy-client-ca.key
I0128 15:06:47.650977   23984 certs.go:315] skipping minikube-user signed cert generation: /home/shubz/.minikube/profiles/minikube/client.key
I0128 15:06:47.674837   23984 certs.go:315] skipping minikube signed cert generation: /home/shubz/.minikube/profiles/minikube/apiserver.key.ad8e880a
I0128 15:06:47.675170   23984 certs.go:315] skipping aggregator signed cert generation: /home/shubz/.minikube/profiles/minikube/proxy-client.key
I0128 15:06:47.675368   23984 certs.go:437] found cert: /home/shubz/.minikube/certs/home/shubz/.minikube/certs/ca-key.pem (1675 bytes)
I0128 15:06:47.675425   23984 certs.go:437] found cert: /home/shubz/.minikube/certs/home/shubz/.minikube/certs/ca.pem (1074 bytes)
I0128 15:06:47.675462   23984 certs.go:437] found cert: /home/shubz/.minikube/certs/home/shubz/.minikube/certs/cert.pem (1119 bytes)
I0128 15:06:47.675499   23984 certs.go:437] found cert: /home/shubz/.minikube/certs/home/shubz/.minikube/certs/key.pem (1675 bytes)
I0128 15:06:47.676389   23984 ssh_runner.go:362] scp /home/shubz/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0128 15:06:47.733487   23984 ssh_runner.go:362] scp /home/shubz/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0128 15:06:47.771421   23984 ssh_runner.go:362] scp /home/shubz/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0128 15:06:47.808807   23984 ssh_runner.go:362] scp /home/shubz/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0128 15:06:47.844170   23984 ssh_runner.go:362] scp /home/shubz/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0128 15:06:47.885292   23984 ssh_runner.go:362] scp /home/shubz/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0128 15:06:47.921082   23984 ssh_runner.go:362] scp /home/shubz/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0128 15:06:47.958696   23984 ssh_runner.go:362] scp /home/shubz/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0128 15:06:47.998176   23984 ssh_runner.go:362] scp /home/shubz/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0128 15:06:48.035896   23984 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0128 15:06:48.063723   23984 ssh_runner.go:195] Run: openssl version
I0128 15:06:48.123087   23984 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0128 15:06:48.152544   23984 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0128 15:06:48.157374   23984 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 13 19:21 /usr/share/ca-certificates/minikubeCA.pem
I0128 15:06:48.157491   23984 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0128 15:06:48.183083   23984 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0128 15:06:48.196289   23984 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0128 15:06:48.213704   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0128 15:06:48.223645   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0128 15:06:48.233209   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0128 15:06:48.242756   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0128 15:06:48.252771   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0128 15:06:48.264326   23984 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0128 15:06:48.274635   23984 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.94.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shubz:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:0s GPUs:}
I0128 15:06:48.274787   23984 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 15:06:48.300692   23984 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0128 15:06:48.313233   23984 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0128 15:06:48.313247   23984 kubeadm.go:636] restartCluster start
I0128 15:06:48.313320   23984 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0128 15:06:48.334008   23984 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0128 15:06:48.374624   23984 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/shubz/.kube/config
I0128 15:06:48.374821   23984 kubeconfig.go:146] "minikube" context is missing from /home/shubz/.kube/config - will repair!
I0128 15:06:48.375231   23984 lock.go:35] WriteFile acquiring /home/shubz/.kube/config: {Name:mk6d78dc41f8db9a7d9ff9c6688e44ce0c9c8f13 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:06:48.464065   23984 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0128 15:06:48.506973   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:48.507105   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:48.587783   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:48.587828   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:48.587955   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:48.605607   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:49.106207   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:49.106460   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:49.129358   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:49.605836   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:49.606079   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:49.627500   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:50.106052   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:50.106311   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:50.129501   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:50.606015   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:50.606221   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:50.629735   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:51.106201   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:51.106403   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:51.130433   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:51.605856   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:51.606026   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:51.629063   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:52.106577   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:52.106804   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:52.130090   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:52.606570   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:52.606819   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:52.626946   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:53.106325   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:53.106542   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:53.130303   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:53.605873   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:53.606211   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:53.627712   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:54.106303   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:54.106569   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:54.131494   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:54.606820   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:54.607110   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:54.630964   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:55.106716   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:55.106927   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:55.131966   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:55.606583   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:55.606789   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:55.631298   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:56.106779   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:56.107039   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:56.131173   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:56.606600   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:56.606827   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:56.629486   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:57.106082   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:57.106345   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:57.131214   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:57.606440   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:57.606666   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:57.628194   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:58.106599   23984 api_server.go:166] Checking apiserver status ...
I0128 15:06:58.106835   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 15:06:58.129594   23984 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 15:06:58.507295   23984 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0128 15:06:58.507339   23984 kubeadm.go:1128] stopping kube-system containers ...
I0128 15:06:58.507551   23984 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 15:06:58.584165   23984 docker.go:469] Stopping containers: [674ed394a7a3 592cb096c157 f279f323a985 e2af9e012937 8a2d25261c21 126cb3c946e7 bac8d43a354d c5a398452fc5 93f59d6501e6 2ad23acbe703 c37f166e7581 6abf903bd805 5fe6d170d7e8 99a434c097d6 13069b45ed0e bc966e355ff5 c063495e9156 957876ae4da3 d225d7306e50 05589e4f57ec f08917bbef6d c7cc2cb2b023 f4d4ad3ce283 5591aaf9dfd0 f72b006a133e c9481793e752]
I0128 15:06:58.584407   23984 ssh_runner.go:195] Run: docker stop 674ed394a7a3 592cb096c157 f279f323a985 e2af9e012937 8a2d25261c21 126cb3c946e7 bac8d43a354d c5a398452fc5 93f59d6501e6 2ad23acbe703 c37f166e7581 6abf903bd805 5fe6d170d7e8 99a434c097d6 13069b45ed0e bc966e355ff5 c063495e9156 957876ae4da3 d225d7306e50 05589e4f57ec f08917bbef6d c7cc2cb2b023 f4d4ad3ce283 5591aaf9dfd0 f72b006a133e c9481793e752
I0128 15:06:58.615029   23984 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0128 15:06:58.732306   23984 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0128 15:06:58.752068   23984 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Jan 24 14:56 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jan 24 15:16 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 24 14:56 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Jan 24 15:16 /etc/kubernetes/scheduler.conf

I0128 15:06:58.752167   23984 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0128 15:06:58.770895   23984 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0128 15:06:58.803820   23984 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0128 15:06:58.832270   23984 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0128 15:06:58.832388   23984 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0128 15:06:58.859839   23984 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0128 15:06:58.875529   23984 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0128 15:06:58.875641   23984 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0128 15:06:58.890639   23984 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0128 15:06:58.906374   23984 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0128 15:06:58.906409   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:00.988439   23984 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (2.081997962s)
I0128 15:07:00.988488   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:02.574178   23984 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.585659932s)
I0128 15:07:02.574200   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:02.895355   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:03.034197   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:03.109676   23984 api_server.go:52] waiting for apiserver process to appear ...
I0128 15:07:03.109765   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:03.124040   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:03.638575   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:04.139075   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:04.639469   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:05.138937   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:05.639080   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:06.139616   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:06.638774   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:07.139383   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:07.638924   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:08.139294   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:08.639362   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:09.138887   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:09.638665   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:10.138818   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:10.638797   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:11.138452   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:11.638577   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:12.139372   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:12.639527   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:13.139016   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:13.639399   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:14.139131   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:14.639223   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:15.138966   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:15.638401   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:16.139258   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:16.639211   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:17.139292   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:17.638819   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:18.139553   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:18.638959   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:18.829520   23984 api_server.go:72] duration metric: took 15.719837369s to wait for apiserver process to appear ...
I0128 15:07:18.829548   23984 api_server.go:88] waiting for apiserver healthz status ...
I0128 15:07:18.829577   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:18.830255   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:18.830321   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:18.830812   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:19.331082   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:19.354208   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:19.830987   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:19.832134   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:20.331981   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:20.333162   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:20.832007   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:20.832999   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:21.331022   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:21.332138   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:21.831175   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:21.832329   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:22.331735   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:22.332973   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:22.831990   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:22.833200   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:23.331222   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:23.332266   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:23.831145   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:23.832351   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:24.331379   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:24.332459   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:24.831605   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:24.832638   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:25.331641   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:25.332693   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:25.831729   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:25.832903   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:26.331813   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:26.332337   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:26.831034   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:26.832159   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:27.331764   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:27.332860   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:27.831545   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:27.832895   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:28.331836   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:28.332978   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:28.830956   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:28.832151   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:29.331094   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:29.332157   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:29.832059   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:29.833172   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:30.331333   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:30.332917   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:30.831015   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:30.832327   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:31.331289   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:31.332104   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:31.831900   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:31.832875   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:32.331515   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:32.332050   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:32.831782   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:32.832236   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:33.331979   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:33.333036   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:33.831213   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:33.832612   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:34.331957   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:34.333226   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:34.831039   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:34.832150   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:35.331647   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:35.332834   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:35.831805   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:35.833031   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:36.331433   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:36.332565   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:36.831446   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:36.832480   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:37.331644   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:37.332699   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:37.831809   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:37.832525   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:38.331366   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:38.332360   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:38.831668   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:38.832822   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:39.331172   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:39.332173   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:39.831362   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:39.832464   23984 api_server.go:269] stopped: https://192.168.94.2:8443/healthz: Get "https://192.168.94.2:8443/healthz": dial tcp 192.168.94.2:8443: connect: connection refused
I0128 15:07:40.331519   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:43.865552   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0128 15:07:43.865583   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 15:07:43.865602   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:43.891897   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0128 15:07:43.891920   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 15:07:44.331735   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:44.354327   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:44.354409   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:44.831128   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:44.858409   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:44.858490   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:45.331282   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:45.349058   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:45.349137   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:45.831618   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:45.846557   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:45.846626   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:46.331045   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:46.353857   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:46.353917   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:46.831602   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:46.852757   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:46.852822   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:47.331574   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:47.353166   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:47.353239   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:47.831205   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:47.848607   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:47.848686   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:48.332017   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:48.340386   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0128 15:07:48.340437   23984 api_server.go:103] status: https://192.168.94.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 15:07:48.831946   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:48.855287   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 200:
ok
I0128 15:07:48.998881   23984 api_server.go:141] control plane version: v1.27.4
I0128 15:07:48.998905   23984 api_server.go:131] duration metric: took 30.169348693s to wait for apiserver health ...
I0128 15:07:48.998914   23984 cni.go:84] Creating CNI manager for ""
I0128 15:07:48.998950   23984 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 15:07:49.292681   23984 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0128 15:07:49.438535   23984 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0128 15:07:49.459381   23984 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0128 15:07:49.592373   23984 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 15:07:49.861471   23984 system_pods.go:59] 7 kube-system pods found
I0128 15:07:49.861543   23984 system_pods.go:61] "coredns-5d78c9869d-wf96w" [53c4ae71-7ecb-462f-9619-a1376ccd498b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 15:07:49.861565   23984 system_pods.go:61] "etcd-minikube" [102f9cc9-89d9-41af-a824-c5118cfe3142] Running
I0128 15:07:49.861592   23984 system_pods.go:61] "kube-apiserver-minikube" [b3fd6a78-8f9e-40c2-bf47-ecd1c8c6808f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0128 15:07:49.861610   23984 system_pods.go:61] "kube-controller-manager-minikube" [78911edb-a0c2-4db0-99ff-a7e7609ebb21] Running
I0128 15:07:49.861633   23984 system_pods.go:61] "kube-proxy-bvmwv" [d7a13a82-e369-408e-ab10-398462ed2f28] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0128 15:07:49.861656   23984 system_pods.go:61] "kube-scheduler-minikube" [0b7ff75d-4d93-4600-96fd-5885538e3fd2] Running
I0128 15:07:49.861771   23984 system_pods.go:61] "storage-provisioner" [09de5948-0fc6-4f30-8413-0395a28f1b39] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 15:07:49.861794   23984 system_pods.go:74] duration metric: took 269.397813ms to wait for pod list to return data ...
I0128 15:07:49.861815   23984 node_conditions.go:102] verifying NodePressure condition ...
I0128 15:07:49.979911   23984 node_conditions.go:122] node storage ephemeral capacity is 112608524Ki
I0128 15:07:49.979965   23984 node_conditions.go:123] node cpu capacity is 4
I0128 15:07:49.980040   23984 node_conditions.go:105] duration metric: took 118.208842ms to run NodePressure ...
I0128 15:07:49.980088   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0128 15:07:52.029261   23984 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.049140112s)
I0128 15:07:52.029296   23984 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0128 15:07:52.053815   23984 ops.go:34] apiserver oom_adj: -16
I0128 15:07:52.053833   23984 kubeadm.go:640] restartCluster took 1m3.740578589s
I0128 15:07:52.053844   23984 kubeadm.go:406] StartCluster complete in 1m3.779217381s
I0128 15:07:52.053864   23984 settings.go:142] acquiring lock: {Name:mk8498562d8915aeb9edcdcaf4cdf104ade048c6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:07:52.054016   23984 settings.go:150] Updating kubeconfig:  /home/shubz/.kube/config
I0128 15:07:52.109239   23984 lock.go:35] WriteFile acquiring /home/shubz/.kube/config: {Name:mk6d78dc41f8db9a7d9ff9c6688e44ce0c9c8f13 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:07:52.109598   23984 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0128 15:07:52.109894   23984 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0128 15:07:52.109990   23984 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0128 15:07:52.110036   23984 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0128 15:07:52.110049   23984 addons.go:240] addon storage-provisioner should already be in state true
I0128 15:07:52.110132   23984 host.go:66] Checking if "minikube" exists ...
I0128 15:07:52.110328   23984 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0128 15:07:52.110347   23984 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0128 15:07:52.166020   23984 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 15:07:52.166578   23984 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 15:07:52.166578   23984 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 15:07:52.386625   23984 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0128 15:07:52.242303   23984 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0128 15:07:52.346930   23984 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0128 15:07:52.486828   23984 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.94.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0128 15:07:52.486930   23984 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0128 15:07:52.609745   23984 out.go:177] 🔎  Verifying Kubernetes components...
I0128 15:07:52.609784   23984 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
W0128 15:07:52.486955   23984 addons.go:240] addon default-storageclass should already be in state true
I0128 15:07:52.609918   23984 host.go:66] Checking if "minikube" exists ...
I0128 15:07:52.610110   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:07:52.786893   23984 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0128 15:07:52.787756   23984 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 15:07:52.823582   23984 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0128 15:07:52.823602   23984 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0128 15:07:52.823719   23984 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 15:07:52.834712   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:07:52.849023   23984 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/shubz/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:07:53.221012   23984 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0128 15:07:53.232250   23984 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 15:07:56.175279   23984 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (4.065644655s)
I0128 15:07:56.175392   23984 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0128 15:07:56.175430   23984 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (3.388512423s)
I0128 15:07:56.175468   23984 api_server.go:52] waiting for apiserver process to appear ...
I0128 15:07:56.175555   23984 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:07:57.357590   23984 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (4.136542136s)
I0128 15:07:57.752356   23984 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.520068627s)
I0128 15:07:57.752398   23984 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.576816724s)
I0128 15:07:57.752424   23984 api_server.go:72] duration metric: took 5.265532426s to wait for apiserver process to appear ...
I0128 15:07:57.752431   23984 api_server.go:88] waiting for apiserver healthz status ...
I0128 15:07:57.752450   23984 api_server.go:253] Checking apiserver healthz at https://192.168.94.2:8443/healthz ...
I0128 15:07:57.805076   23984 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0128 15:07:57.761942   23984 api_server.go:279] https://192.168.94.2:8443/healthz returned 200:
ok
I0128 15:07:57.806864   23984 api_server.go:141] control plane version: v1.27.4
I0128 15:07:57.838928   23984 api_server.go:131] duration metric: took 86.431045ms to wait for apiserver health ...
I0128 15:07:57.838925   23984 addons.go:502] enable addons completed in 5.728879784s: enabled=[default-storageclass storage-provisioner]
I0128 15:07:57.838975   23984 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 15:07:57.864150   23984 system_pods.go:59] 7 kube-system pods found
I0128 15:07:57.864210   23984 system_pods.go:61] "coredns-5d78c9869d-wf96w" [53c4ae71-7ecb-462f-9619-a1376ccd498b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 15:07:57.864232   23984 system_pods.go:61] "etcd-minikube" [102f9cc9-89d9-41af-a824-c5118cfe3142] Running
I0128 15:07:57.864254   23984 system_pods.go:61] "kube-apiserver-minikube" [b3fd6a78-8f9e-40c2-bf47-ecd1c8c6808f] Running
I0128 15:07:57.864270   23984 system_pods.go:61] "kube-controller-manager-minikube" [78911edb-a0c2-4db0-99ff-a7e7609ebb21] Running
I0128 15:07:57.864293   23984 system_pods.go:61] "kube-proxy-bvmwv" [d7a13a82-e369-408e-ab10-398462ed2f28] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0128 15:07:57.864310   23984 system_pods.go:61] "kube-scheduler-minikube" [0b7ff75d-4d93-4600-96fd-5885538e3fd2] Running
I0128 15:07:57.864332   23984 system_pods.go:61] "storage-provisioner" [09de5948-0fc6-4f30-8413-0395a28f1b39] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 15:07:57.864349   23984 system_pods.go:74] duration metric: took 25.352259ms to wait for pod list to return data ...
I0128 15:07:57.864372   23984 kubeadm.go:581] duration metric: took 5.377477872s to wait for : map[apiserver:true system_pods:true] ...
I0128 15:07:57.864405   23984 node_conditions.go:102] verifying NodePressure condition ...
I0128 15:07:57.875664   23984 node_conditions.go:122] node storage ephemeral capacity is 112608524Ki
I0128 15:07:57.875714   23984 node_conditions.go:123] node cpu capacity is 4
I0128 15:07:57.875744   23984 node_conditions.go:105] duration metric: took 11.326717ms to run NodePressure ...
I0128 15:07:57.875790   23984 start.go:228] waiting for startup goroutines ...
I0128 15:07:57.875816   23984 start.go:233] waiting for cluster config update ...
I0128 15:07:57.875846   23984 start.go:242] writing updated cluster config ...
I0128 15:07:57.877367   23984 ssh_runner.go:195] Run: rm -f paused
I0128 15:08:01.648985   23984 start.go:600] kubectl: 1.29.1, cluster: 1.27.4 (minor skew: 2)
I0128 15:08:01.770152   23984 out.go:177] 
W0128 15:08:01.814932   23984 out.go:239] ❗  /usr/local/bin/kubectl is version 1.29.1, which may have incompatibilities with Kubernetes 1.27.4.
I0128 15:08:01.948026   23984 out.go:177]     ▪ Want kubectl v1.27.4? Try 'minikube kubectl -- get pods -A'
I0128 15:08:02.004027   23984 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 28 09:36:26 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 28 09:36:33 minikube systemd[1]: docker.service: Deactivated successfully.
Jan 28 09:36:33 minikube systemd[1]: Stopped Docker Application Container Engine.
Jan 28 09:36:33 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 28 09:36:34 minikube dockerd[505]: time="2024-01-28T09:36:34.748546345Z" level=info msg="Starting up"
Jan 28 09:36:35 minikube dockerd[505]: time="2024-01-28T09:36:35.090119504Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jan 28 09:36:36 minikube dockerd[505]: time="2024-01-28T09:36:36.964467363Z" level=info msg="Loading containers: start."
Jan 28 09:36:38 minikube dockerd[505]: time="2024-01-28T09:36:38.571895193Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 28 09:36:38 minikube dockerd[505]: time="2024-01-28T09:36:38.800648444Z" level=info msg="Loading containers: done."
Jan 28 09:36:39 minikube dockerd[505]: time="2024-01-28T09:36:39.538207902Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Jan 28 09:36:39 minikube dockerd[505]: time="2024-01-28T09:36:39.547306932Z" level=info msg="Daemon has completed initialization"
Jan 28 09:36:39 minikube dockerd[505]: time="2024-01-28T09:36:39.872518192Z" level=info msg="API listen on /var/run/docker.sock"
Jan 28 09:36:39 minikube dockerd[505]: time="2024-01-28T09:36:39.872612733Z" level=info msg="API listen on [::]:2376"
Jan 28 09:36:39 minikube systemd[1]: Started Docker Application Container Engine.
Jan 28 09:36:40 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Start docker client with request timeout 0s"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Loaded network plugin cni"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Docker Info: &{ID:64179618-ea1f-4b92-878a-d2b58896889a Containers:28 ContainersRunning:0 ContainersPaused:0 ContainersStopped:28 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:36 SystemTime:2024-01-28T09:36:42.594403624Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:5.15.0-91-generic OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0002c4070 NCPU:4 MemTotal:6081265664 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Setting cgroupDriver systemd"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 28 09:36:42 minikube cri-dockerd[751]: time="2024-01-28T09:36:42Z" level=info msg="Start cri-dockerd grpc backend"
Jan 28 09:36:42 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 28 09:37:07 minikube cri-dockerd[751]: time="2024-01-28T09:37:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mysql-7d94d9bf46-tfsqm_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9f3a4d94e84a0827e8cdf7678d781e11c2e71186ecb915458eb155e51ba7f8d9\""
Jan 28 09:37:09 minikube cri-dockerd[751]: time="2024-01-28T09:37:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-wf96w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6abf903bd805884464936026c03b3dc58d1a71e87171ca6de16d4dcaf9686d24\""
Jan 28 09:37:09 minikube cri-dockerd[751]: time="2024-01-28T09:37:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-wf96w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d225d7306e50a1cb250f41eba9f40dedf70d853db06677f831a2ce8e2ba6634f\""
Jan 28 09:37:11 minikube cri-dockerd[751]: time="2024-01-28T09:37:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"two-tier-app-57f45bf559-tr8sv_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8fa071397c95c5034c723d391c210a742b7c6710fd1eb7e6315b50c212370c57\""
Jan 28 09:37:12 minikube cri-dockerd[751]: time="2024-01-28T09:37:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-wf96w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6abf903bd805884464936026c03b3dc58d1a71e87171ca6de16d4dcaf9686d24\""
Jan 28 09:37:12 minikube cri-dockerd[751]: time="2024-01-28T09:37:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-wf96w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d225d7306e50a1cb250f41eba9f40dedf70d853db06677f831a2ce8e2ba6634f\""
Jan 28 09:37:16 minikube cri-dockerd[751]: time="2024-01-28T09:37:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0ee64fdae2f1e1452e7075190dd3cc9dfc99fac59f2f2100ee6d3916be091d5a/resolv.conf as [nameserver 192.168.94.1 options trust-ad ndots:0 edns0]"
Jan 28 09:37:16 minikube cri-dockerd[751]: time="2024-01-28T09:37:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/66a99bd0aa77dd19fea3f680b7070c5b647ee06107b6c6f5b60b23e449069c0d/resolv.conf as [nameserver 192.168.94.1 options edns0 trust-ad ndots:0]"
Jan 28 09:37:16 minikube cri-dockerd[751]: time="2024-01-28T09:37:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b81ffaad9050acedf2ce50ad5023c2313401c9f01aae571bb743307c2cb6dc7/resolv.conf as [nameserver 192.168.94.1 options edns0 trust-ad ndots:0]"
Jan 28 09:37:16 minikube cri-dockerd[751]: time="2024-01-28T09:37:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dfe4659d9aec5de80e20539c8023a8e998a5f0acfd38d823b27f51f2c641a1e4/resolv.conf as [nameserver 192.168.94.1 options edns0 trust-ad ndots:0]"
Jan 28 09:37:17 minikube cri-dockerd[751]: time="2024-01-28T09:37:17Z" level=error msg="Failed to retrieve checkpoint for sandbox d225d7306e50a1cb250f41eba9f40dedf70d853db06677f831a2ce8e2ba6634f: checkpoint is not found"
Jan 28 09:37:17 minikube cri-dockerd[751]: time="2024-01-28T09:37:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-wf96w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6abf903bd805884464936026c03b3dc58d1a71e87171ca6de16d4dcaf9686d24\""
Jan 28 09:37:44 minikube cri-dockerd[751]: time="2024-01-28T09:37:44Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 28 09:37:49 minikube cri-dockerd[751]: time="2024-01-28T09:37:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5ec36d2784193429ebe04932c098da976aa7bba784a5a81b2c0271d0f7407afa/resolv.conf as [nameserver 192.168.94.1 options ndots:0 edns0 trust-ad]"
Jan 28 09:37:50 minikube cri-dockerd[751]: time="2024-01-28T09:37:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84ae12f0602fd24aabbabc1f8423575b069e8efa41d1216c45d47ee51ef3a0dc/resolv.conf as [nameserver 192.168.94.1 options ndots:0 edns0 trust-ad]"
Jan 28 09:37:50 minikube cri-dockerd[751]: time="2024-01-28T09:37:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ac2576cbb14b29c5f0d5b09eb30bb1a38fe970790a10901d7f00fd06cc25b60/resolv.conf as [nameserver 192.168.94.1 options trust-ad ndots:0 edns0]"
Jan 28 09:38:59 minikube dockerd[505]: time="2024-01-28T09:38:59.526431037Z" level=info msg="ignoring event" container=f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 09:46:18 minikube cri-dockerd[751]: time="2024-01-28T09:46:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/870cacb2e0e6434446594ed1800327c7125e47d0b1925002fede72fc39bf614a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 09:46:21 minikube cri-dockerd[751]: time="2024-01-28T09:46:21Z" level=info msg="Stop pulling image mysql:latest: Status: Image is up to date for mysql:latest"
Jan 28 09:47:29 minikube cri-dockerd[751]: time="2024-01-28T09:47:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9dc9f85f8a728ac39bc11bb099852595930ca7a4e82f873ce4cc1b90a0f69993/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 09:47:43 minikube cri-dockerd[751]: time="2024-01-28T09:47:43Z" level=info msg="Pulling image shubzz/flask:latest: 2f44b7a888fa: Download complete "
Jan 28 09:47:53 minikube cri-dockerd[751]: time="2024-01-28T09:47:53Z" level=info msg="Pulling image shubzz/flask:latest: 94bcf91a251a: Pull complete "
Jan 28 09:48:03 minikube cri-dockerd[751]: time="2024-01-28T09:48:03Z" level=info msg="Pulling image shubzz/flask:latest: a0fa17d5af88: Extracting [=============>                                     ]  22.84MB/86.81MB"
Jan 28 09:48:13 minikube cri-dockerd[751]: time="2024-01-28T09:48:13Z" level=info msg="Pulling image shubzz/flask:latest: 17ba4d60c188: Extracting [==================================================>]     203B/203B"
Jan 28 09:48:18 minikube cri-dockerd[751]: time="2024-01-28T09:48:18Z" level=info msg="Stop pulling image shubzz/flask:latest: Status: Downloaded newer image for shubzz/flask:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                  CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
64a326c3b7e10       shubzz/flask@sha256:27a87df46361ceabce945e028acc4110f1718fe85298f9e8fc355078cf884018   4 minutes ago       Running             two-tier-app              0                   9dc9f85f8a728       two-tier-app-5467879f76-wsm25
14cf2d84d1821       mysql@sha256:d7c20c5ba268c558f4fac62977f8c7125bde0630ff8946b08dde44135ef40df3          6 minutes ago       Running             mysql                     0                   870cacb2e0e64       mysql-6dd4c8b9c4-pg8h9
cd7c656fd96d1       6e38f40d628db                                                                          11 minutes ago      Running             storage-provisioner       8                   5ec36d2784193       storage-provisioner
6692627bd5c80       ead0a4a53df89                                                                          14 minutes ago      Running             coredns                   3                   2ac2576cbb14b       coredns-5d78c9869d-wf96w
d816df1e7fe4b       6848d7eda0341                                                                          14 minutes ago      Running             kube-proxy                3                   84ae12f0602fd       kube-proxy-bvmwv
f3d2b800613be       6e38f40d628db                                                                          14 minutes ago      Exited              storage-provisioner       7                   5ec36d2784193       storage-provisioner
4fcc6f93e0966       f466468864b7a                                                                          15 minutes ago      Running             kube-controller-manager   4                   0ee64fdae2f1e       kube-controller-manager-minikube
ab59dee2daf1e       e7972205b6614                                                                          15 minutes ago      Running             kube-apiserver            3                   9b81ffaad9050       kube-apiserver-minikube
b692f21cab9e7       86b6af7dd652c                                                                          15 minutes ago      Running             etcd                      3                   dfe4659d9aec5       etcd-minikube
7221579667df0       98ef2570f3cde                                                                          15 minutes ago      Running             kube-scheduler            3                   66a99bd0aa77d       kube-scheduler-minikube
674ed394a7a36       f466468864b7a                                                                          38 hours ago        Exited              kube-controller-manager   3                   5fe6d170d7e8f       kube-controller-manager-minikube
f279f323a985d       ead0a4a53df89                                                                          38 hours ago        Exited              coredns                   2                   6abf903bd8058       coredns-5d78c9869d-wf96w
e2af9e0129377       6848d7eda0341                                                                          38 hours ago        Exited              kube-proxy                2                   2ad23acbe7030       kube-proxy-bvmwv
8a2d25261c212       98ef2570f3cde                                                                          38 hours ago        Exited              kube-scheduler            2                   c37f166e75813       kube-scheduler-minikube
126cb3c946e75       e7972205b6614                                                                          38 hours ago        Exited              kube-apiserver            2                   bc966e355ff55       kube-apiserver-minikube
bac8d43a354db       86b6af7dd652c                                                                          38 hours ago        Exited              etcd                      2                   13069b45ed0e3       etcd-minikube

* 
* ==> coredns [6692627bd5c8] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4c7f44b73086be760ec9e64204f63c5cc5a952c8c1c55ba0b41d8fc3315ce3c7d0259d04847cb8b4561043d4549603f3bccfd9b397eeb814eef159d244d26f39
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:45627 - 52824 "HINFO IN 3878976731453989732.8238433818687476197. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.049050444s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [f279f323a985] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4c7f44b73086be760ec9e64204f63c5cc5a952c8c1c55ba0b41d8fc3315ce3c7d0259d04847cb8b4561043d4549603f3bccfd9b397eeb814eef159d244d26f39
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:33586 - 61136 "HINFO IN 695882847915801730.4084493668110296937. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.010187065s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_24T20_26_57_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 24 Jan 2024 14:56:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Jan 2024 09:52:57 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Jan 2024 09:48:39 +0000   Wed, 24 Jan 2024 14:56:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Jan 2024 09:48:39 +0000   Wed, 24 Jan 2024 14:56:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Jan 2024 09:48:39 +0000   Wed, 24 Jan 2024 14:56:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Jan 2024 09:48:39 +0000   Wed, 24 Jan 2024 14:56:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.94.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  112608524Ki
  hugepages-2Mi:      0
  memory:             5938736Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  112608524Ki
  hugepages-2Mi:      0
  memory:             5938736Ki
  pods:               110
System Info:
  Machine ID:                 023ec280e59b438ab74c5d9e155a49e7
  System UUID:                3d506a80-9687-45d7-987e-4550de9ab87e
  Boot ID:                    abc80ec2-0ab4-4324-8df1-9d608d17ddfc
  Kernel Version:             5.15.0-91-generic
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     mysql-6dd4c8b9c4-pg8h9              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m44s
  default                     two-tier-app-5467879f76-wsm25       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m32s
  kube-system                 coredns-5d78c9869d-wf96w            100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     3d18h
  kube-system                 etcd-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3d18h
  kube-system                 kube-apiserver-minikube             250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d18h
  kube-system                 kube-controller-manager-minikube    200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d18h
  kube-system                 kube-proxy-bvmwv                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d18h
  kube-system                 kube-scheduler-minikube             100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d18h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d18h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 3d18h                  kube-proxy       
  Normal   Starting                 14m                    kube-proxy       
  Normal   Starting                 37h                    kube-proxy       
  Normal   Starting                 3d18h                  kube-proxy       
  Normal   NodeHasSufficientPID     3d18h (x7 over 3d18h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    3d18h (x8 over 3d18h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced  3d18h                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  3d18h (x8 over 3d18h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                 3d18h                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  3d18h                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    3d18h                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     3d18h                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  3d18h                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           3d18h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 3d18h                  kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  3d18h                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  3d18h (x8 over 3d18h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    3d18h (x8 over 3d18h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     3d18h (x7 over 3d18h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           3d18h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady             37h                    kubelet          Node minikube status is now: NodeNotReady
  Warning  ContainerGCFailed        37h                    kubelet          rpc error: code = Unavailable desc = connection error: desc = "transport: failed to write client preface: write unix @->/run/cri-dockerd.sock: write: broken pipe"
  Normal   RegisteredNode           37h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 15m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  15m (x8 over 15m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    15m (x8 over 15m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     15m (x7 over 15m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  15m                    kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           14m                    node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.072321] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.756080] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +8.515082] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[ +27.156713] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x02 PREC=0x40 TTL=1 ID=52720 PROTO=2 
[Jan28 09:23] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x12 PREC=0xA0 TTL=1 ID=6414 PROTO=2 
[Jan28 09:25] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x1C PREC=0xE0 TTL=1 ID=25899 PROTO=2 
[Jan28 09:26] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:27] atkbd serio0: Unknown key pressed (translated set 2, code 0x8d on isa0060/serio0).
[  +0.000020] atkbd serio0: Use 'setkeycodes e00d <keycode>' to make it known.
[Jan28 09:28] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x10 PREC=0x80 TTL=1 ID=45384 PROTO=2 
[ +13.193120] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:9a:ae:99:89:af:e2:08:00 SRC=192.168.1.102 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +5.883978] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:9a:ae:99:89:af:e2:08:00 SRC=192.168.1.102 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +7.370203] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.251 LEN=32 TOS=0x1C PREC=0x40 TTL=1 ID=58190 PROTO=2 
[  +0.253577] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[ +10.446306] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.251 LEN=32 TOS=0x08 PREC=0x80 TTL=1 ID=25937 PROTO=2 
[  +0.006181] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.497418] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:ca:dd:bc:eb:75:22:08:00 SRC=192.168.1.101 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:30] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x16 PREC=0x20 TTL=1 ID=64869 PROTO=2 
[  +0.486977] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:ca:dd:bc:eb:75:22:08:00 SRC=192.168.1.101 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:32] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x10 PREC=0x80 TTL=1 ID=18819 PROTO=2 
[  +0.870859] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:34] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x80 TTL=1 ID=38304 PROTO=2 
[Jan28 09:35] Bluetooth: hci0: Timed out waiting for suspend events
[  +0.000032] Bluetooth: hci0: Suspend timeout bit: 6
[  +0.000046] Bluetooth: hci0: Suspend notifier action (3) failed: -110
[  +1.842357] done.
[  +2.015315] ata1.00: ATA Identify Device Log not supported
[  +0.015222] ata1.00: ATA Identify Device Log not supported
[  +1.856414] atkbd serio0: Unknown key pressed (translated set 2, code 0x8d on isa0060/serio0).
[  +0.000021] atkbd serio0: Use 'setkeycodes e00d <keycode>' to make it known.
[Jan28 09:37] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x80 TTL=1 ID=38387 PROTO=2 
[  +5.429676] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[ +10.325736] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.251 LEN=32 TOS=0x0A PREC=0x20 TTL=1 ID=18167 PROTO=2 
[  +0.018911] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.191838] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:39] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x80 TTL=1 ID=57616 PROTO=2 
[  +8.796002] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52317 PROTO=2 
[Jan28 09:41] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x20 TTL=1 ID=11566 PROTO=2 
[  +8.800112] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52319 PROTO=2 
[Jan28 09:43] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x40 TTL=1 ID=31051 PROTO=2 
[  +5.792223] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52328 PROTO=2 
[Jan28 09:45] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x80 TTL=1 ID=50536 PROTO=2 
[  +9.282200] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52329 PROTO=2 
[Jan28 09:47] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x10 PREC=0x80 TTL=1 ID=4486 PROTO=2 
[  +6.279704] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52330 PROTO=2 
[ +21.190139] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:48] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.251 LEN=32 TOS=0x0A PREC=0x20 TTL=1 ID=53389 PROTO=2 
[  +0.241076] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52331 PROTO=2 
[ +56.443034] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Jan28 09:49] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.512837] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:04:c8:07:36:1f:4e:08:00 SRC=192.168.1.106 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[ +32.258729] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x40 TTL=1 ID=23971 PROTO=2 
[  +8.276738] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52332 PROTO=2 
[Jan28 09:50] [UFW BLOCK] IN=wlp7s0 OUT= MAC=08:3e:8e:1d:c7:ff:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=192.168.1.120 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=17393 DF PROTO=TCP SPT=63455 DPT=7680 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.997251] [UFW BLOCK] IN=wlp7s0 OUT= MAC=08:3e:8e:1d:c7:ff:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=192.168.1.120 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=17394 DF PROTO=TCP SPT=63455 DPT=7680 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +2.003633] [UFW BLOCK] IN=wlp7s0 OUT= MAC=08:3e:8e:1d:c7:ff:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=192.168.1.120 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=17395 DF PROTO=TCP SPT=63455 DPT=7680 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +4.003819] [UFW BLOCK] IN=wlp7s0 OUT= MAC=08:3e:8e:1d:c7:ff:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=192.168.1.120 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=17396 DF PROTO=TCP SPT=63455 DPT=7680 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +8.003828] [UFW BLOCK] IN=wlp7s0 OUT= MAC=08:3e:8e:1d:c7:ff:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=192.168.1.120 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=17397 DF PROTO=TCP SPT=63455 DPT=7680 WINDOW=64240 RES=0x00 SYN URGP=0 
[Jan28 09:51] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:01:c0:25:2f:94:c3:84:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x04 PREC=0x80 TTL=1 ID=43456 PROTO=2 
[  +1.287028] [UFW BLOCK] IN=wlp7s0 OUT= MAC=01:00:5e:00:00:fb:64:d6:9a:5c:7a:ec:08:00 SRC=192.168.1.104 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=52333 PROTO=2 

* 
* ==> etcd [b692f21cab9e] <==
* {"level":"warn","ts":"2024-01-28T09:48:35.593Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:35.001Z","time spent":"591.83199ms","remote":"127.0.0.1:54590","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-01-28T09:48:37.111Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.571424ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:37.111Z","caller":"traceutil/trace.go:171","msg":"trace[2068865479] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3967; }","duration":"109.807126ms","start":"2024-01-28T09:48:37.002Z","end":"2024-01-28T09:48:37.111Z","steps":["trace[2068865479] 'range keys from in-memory index tree'  (duration: 109.348777ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:38.899Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.002585ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549726 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.94.2\" mod_revision:3961 > success:<request_put:<key:\"/registry/masterleases/192.168.94.2\" value_size:65 lease:6571751653558549724 >> failure:<request_range:<key:\"/registry/masterleases/192.168.94.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T09:48:38.900Z","caller":"traceutil/trace.go:171","msg":"trace[2133947353] transaction","detail":"{read_only:false; response_revision:3968; number_of_response:1; }","duration":"262.284102ms","start":"2024-01-28T09:48:38.637Z","end":"2024-01-28T09:48:38.899Z","steps":["trace[2133947353] 'process raft request'  (duration: 139.080543ms)","trace[2133947353] 'compare'  (duration: 122.849304ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:39.401Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"156.291013ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549731 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3967 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T09:48:39.401Z","caller":"traceutil/trace.go:171","msg":"trace[757524321] linearizableReadLoop","detail":"{readStateIndex:4862; appliedIndex:4861; }","duration":"403.580588ms","start":"2024-01-28T09:48:38.998Z","end":"2024-01-28T09:48:39.401Z","steps":["trace[757524321] 'read index received'  (duration: 247.212751ms)","trace[757524321] 'applied index is now lower than readState.Index'  (duration: 156.365661ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:39.402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"403.804078ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:39.402Z","caller":"traceutil/trace.go:171","msg":"trace[882787130] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3969; }","duration":"403.855758ms","start":"2024-01-28T09:48:38.998Z","end":"2024-01-28T09:48:39.402Z","steps":["trace[882787130] 'agreement among raft nodes before linearized reading'  (duration: 403.737772ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:39.402Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:38.998Z","time spent":"403.93726ms","remote":"127.0.0.1:54592","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-01-28T09:48:39.402Z","caller":"traceutil/trace.go:171","msg":"trace[688696439] transaction","detail":"{read_only:false; response_revision:3969; number_of_response:1; }","duration":"473.990361ms","start":"2024-01-28T09:48:38.928Z","end":"2024-01-28T09:48:39.402Z","steps":["trace[688696439] 'process raft request'  (duration: 317.072824ms)","trace[688696439] 'compare'  (duration: 156.118175ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:39.402Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:38.928Z","time spent":"474.138867ms","remote":"127.0.0.1:54502","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3967 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-01-28T09:48:39.849Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.931932ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549732 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:3931 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:4765 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T09:48:39.849Z","caller":"traceutil/trace.go:171","msg":"trace[314271435] linearizableReadLoop","detail":"{readStateIndex:4863; appliedIndex:4862; }","duration":"343.576047ms","start":"2024-01-28T09:48:39.505Z","end":"2024-01-28T09:48:39.849Z","steps":["trace[314271435] 'read index received'  (duration: 218.341397ms)","trace[314271435] 'applied index is now lower than readState.Index'  (duration: 125.232021ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T09:48:39.849Z","caller":"traceutil/trace.go:171","msg":"trace[923441499] transaction","detail":"{read_only:false; response_revision:3970; number_of_response:1; }","duration":"682.658763ms","start":"2024-01-28T09:48:39.166Z","end":"2024-01-28T09:48:39.849Z","steps":["trace[923441499] 'process raft request'  (duration: 557.412197ms)","trace[923441499] 'compare'  (duration: 124.364771ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:39.849Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"343.836212ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2024-01-28T09:48:39.849Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:39.166Z","time spent":"682.811291ms","remote":"127.0.0.1:54516","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4799,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:3931 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:4765 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >"}
{"level":"info","ts":"2024-01-28T09:48:39.849Z","caller":"traceutil/trace.go:171","msg":"trace[2047742441] range","detail":"{range_begin:/registry/leases/; range_end:/registry/leases0; response_count:0; response_revision:3970; }","duration":"343.918785ms","start":"2024-01-28T09:48:39.505Z","end":"2024-01-28T09:48:39.849Z","steps":["trace[2047742441] 'agreement among raft nodes before linearized reading'  (duration: 343.705671ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:39.849Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:39.505Z","time spent":"344.036072ms","remote":"127.0.0.1:54634","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":2,"response size":31,"request content":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true "}
{"level":"warn","ts":"2024-01-28T09:48:40.268Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"257.643202ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:40.268Z","caller":"traceutil/trace.go:171","msg":"trace[1959826262] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3970; }","duration":"258.038669ms","start":"2024-01-28T09:48:40.010Z","end":"2024-01-28T09:48:40.268Z","steps":["trace[1959826262] 'range keys from in-memory index tree'  (duration: 257.355355ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:41.545Z","caller":"traceutil/trace.go:171","msg":"trace[1182113183] transaction","detail":"{read_only:false; response_revision:3971; number_of_response:1; }","duration":"129.673597ms","start":"2024-01-28T09:48:41.416Z","end":"2024-01-28T09:48:41.545Z","steps":["trace[1182113183] 'process raft request'  (duration: 129.199679ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:41.868Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"163.704201ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549740 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:3963 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T09:48:41.869Z","caller":"traceutil/trace.go:171","msg":"trace[783498040] transaction","detail":"{read_only:false; response_revision:3972; number_of_response:1; }","duration":"424.89567ms","start":"2024-01-28T09:48:41.444Z","end":"2024-01-28T09:48:41.869Z","steps":["trace[783498040] 'process raft request'  (duration: 260.129951ms)","trace[783498040] 'compare'  (duration: 163.275889ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:41.869Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:41.444Z","time spent":"425.132588ms","remote":"127.0.0.1:54634","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:3963 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-01-28T09:48:42.201Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.496706ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:42.201Z","caller":"traceutil/trace.go:171","msg":"trace[1294596195] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3972; }","duration":"194.612028ms","start":"2024-01-28T09:48:42.006Z","end":"2024-01-28T09:48:42.201Z","steps":["trace[1294596195] 'range keys from in-memory index tree'  (duration: 194.358057ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:43.804Z","caller":"traceutil/trace.go:171","msg":"trace[1504544932] linearizableReadLoop","detail":"{readStateIndex:4866; appliedIndex:4865; }","duration":"248.695688ms","start":"2024-01-28T09:48:43.556Z","end":"2024-01-28T09:48:43.804Z","steps":["trace[1504544932] 'read index received'  (duration: 248.41451ms)","trace[1504544932] 'applied index is now lower than readState.Index'  (duration: 279.015µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:43.805Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"249.454167ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-01-28T09:48:43.806Z","caller":"traceutil/trace.go:171","msg":"trace[156457876] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3973; }","duration":"249.981982ms","start":"2024-01-28T09:48:43.556Z","end":"2024-01-28T09:48:43.806Z","steps":["trace[156457876] 'agreement among raft nodes before linearized reading'  (duration: 249.261477ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:43.807Z","caller":"traceutil/trace.go:171","msg":"trace[1076410321] transaction","detail":"{read_only:false; response_revision:3973; number_of_response:1; }","duration":"254.23073ms","start":"2024-01-28T09:48:43.553Z","end":"2024-01-28T09:48:43.807Z","steps":["trace[1076410321] 'process raft request'  (duration: 251.661413ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:44.115Z","caller":"traceutil/trace.go:171","msg":"trace[759408389] linearizableReadLoop","detail":"{readStateIndex:4867; appliedIndex:4866; }","duration":"185.09978ms","start":"2024-01-28T09:48:43.929Z","end":"2024-01-28T09:48:44.115Z","steps":["trace[759408389] 'read index received'  (duration: 98.037711ms)","trace[759408389] 'applied index is now lower than readState.Index'  (duration: 87.06028ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T09:48:44.115Z","caller":"traceutil/trace.go:171","msg":"trace[2141115339] transaction","detail":"{read_only:false; response_revision:3974; number_of_response:1; }","duration":"294.05596ms","start":"2024-01-28T09:48:43.821Z","end":"2024-01-28T09:48:44.115Z","steps":["trace[2141115339] 'process raft request'  (duration: 206.720836ms)","trace[2141115339] 'compare'  (duration: 86.654861ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:44.115Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.702632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-28T09:48:44.115Z","caller":"traceutil/trace.go:171","msg":"trace[1509445146] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:3974; }","duration":"185.86965ms","start":"2024-01-28T09:48:43.929Z","end":"2024-01-28T09:48:44.115Z","steps":["trace[1509445146] 'agreement among raft nodes before linearized reading'  (duration: 185.634212ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:44.519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"236.658266ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549748 > lease_revoke:<id:5b338d4f6e717cba>","response":"size:29"}
{"level":"info","ts":"2024-01-28T09:48:44.519Z","caller":"traceutil/trace.go:171","msg":"trace[467335641] linearizableReadLoop","detail":"{readStateIndex:4868; appliedIndex:4867; }","duration":"404.586912ms","start":"2024-01-28T09:48:44.115Z","end":"2024-01-28T09:48:44.519Z","steps":["trace[467335641] 'read index received'  (duration: 167.390252ms)","trace[467335641] 'applied index is now lower than readState.Index'  (duration: 237.191348ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T09:48:44.519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"520.374534ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:44.520Z","caller":"traceutil/trace.go:171","msg":"trace[1309186184] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3974; }","duration":"520.448459ms","start":"2024-01-28T09:48:43.999Z","end":"2024-01-28T09:48:44.519Z","steps":["trace[1309186184] 'agreement among raft nodes before linearized reading'  (duration: 520.282961ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:44.520Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:43.999Z","time spent":"520.537511ms","remote":"127.0.0.1:54592","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-01-28T09:48:46.345Z","caller":"traceutil/trace.go:171","msg":"trace[1133346725] transaction","detail":"{read_only:false; response_revision:3975; number_of_response:1; }","duration":"215.104347ms","start":"2024-01-28T09:48:46.130Z","end":"2024-01-28T09:48:46.345Z","steps":["trace[1133346725] 'process raft request'  (duration: 214.581073ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:47.423Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"419.345168ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-28T09:48:47.423Z","caller":"traceutil/trace.go:171","msg":"trace[1648100449] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3975; }","duration":"419.538442ms","start":"2024-01-28T09:48:47.004Z","end":"2024-01-28T09:48:47.423Z","steps":["trace[1648100449] 'range keys from in-memory index tree'  (duration: 419.136579ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:47.424Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:48:47.004Z","time spent":"419.707586ms","remote":"127.0.0.1:54590","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-01-28T09:48:48.555Z","caller":"traceutil/trace.go:171","msg":"trace[46740371] transaction","detail":"{read_only:false; response_revision:3976; number_of_response:1; }","duration":"199.06889ms","start":"2024-01-28T09:48:48.356Z","end":"2024-01-28T09:48:48.555Z","steps":["trace[46740371] 'process raft request'  (duration: 198.953252ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:48:48.812Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"156.367658ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751653558549767 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.94.2\" mod_revision:3968 > success:<request_put:<key:\"/registry/masterleases/192.168.94.2\" value_size:65 lease:6571751653558549765 >> failure:<request_range:<key:\"/registry/masterleases/192.168.94.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-28T09:48:48.812Z","caller":"traceutil/trace.go:171","msg":"trace[98156005] transaction","detail":"{read_only:false; response_revision:3977; number_of_response:1; }","duration":"190.906768ms","start":"2024-01-28T09:48:48.621Z","end":"2024-01-28T09:48:48.812Z","steps":["trace[98156005] 'process raft request'  (duration: 34.399564ms)","trace[98156005] 'compare'  (duration: 156.224089ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T09:48:52.769Z","caller":"traceutil/trace.go:171","msg":"trace[600540977] transaction","detail":"{read_only:false; response_revision:3980; number_of_response:1; }","duration":"132.537237ms","start":"2024-01-28T09:48:52.637Z","end":"2024-01-28T09:48:52.769Z","steps":["trace[600540977] 'process raft request'  (duration: 132.298261ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:54.920Z","caller":"traceutil/trace.go:171","msg":"trace[1150448222] transaction","detail":"{read_only:false; response_revision:3982; number_of_response:1; }","duration":"101.324006ms","start":"2024-01-28T09:48:54.819Z","end":"2024-01-28T09:48:54.920Z","steps":["trace[1150448222] 'process raft request'  (duration: 100.980727ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:57.069Z","caller":"traceutil/trace.go:171","msg":"trace[242077935] transaction","detail":"{read_only:false; response_revision:3983; number_of_response:1; }","duration":"135.533085ms","start":"2024-01-28T09:48:56.933Z","end":"2024-01-28T09:48:57.069Z","steps":["trace[242077935] 'process raft request'  (duration: 134.717493ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:48:58.805Z","caller":"traceutil/trace.go:171","msg":"trace[328603321] transaction","detail":"{read_only:false; response_revision:3984; number_of_response:1; }","duration":"141.323944ms","start":"2024-01-28T09:48:58.663Z","end":"2024-01-28T09:48:58.805Z","steps":["trace[328603321] 'process raft request'  (duration: 80.277685ms)","trace[328603321] 'compare'  (duration: 60.814099ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T09:49:03.498Z","caller":"traceutil/trace.go:171","msg":"trace[717426650] transaction","detail":"{read_only:false; response_revision:3988; number_of_response:1; }","duration":"266.629151ms","start":"2024-01-28T09:49:03.231Z","end":"2024-01-28T09:49:03.498Z","steps":["trace[717426650] 'process raft request'  (duration: 266.395593ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:49:07.681Z","caller":"traceutil/trace.go:171","msg":"trace[797090050] transaction","detail":"{read_only:false; response_revision:3991; number_of_response:1; }","duration":"112.063179ms","start":"2024-01-28T09:49:07.569Z","end":"2024-01-28T09:49:07.681Z","steps":["trace[797090050] 'process raft request'  (duration: 111.748592ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:49:12.596Z","caller":"traceutil/trace.go:171","msg":"trace[260806692] transaction","detail":"{read_only:false; response_revision:3995; number_of_response:1; }","duration":"108.773874ms","start":"2024-01-28T09:49:12.487Z","end":"2024-01-28T09:49:12.595Z","steps":["trace[260806692] 'process raft request'  (duration: 108.468938ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:50:09.514Z","caller":"traceutil/trace.go:171","msg":"trace[505934184] transaction","detail":"{read_only:false; response_revision:4040; number_of_response:1; }","duration":"105.876647ms","start":"2024-01-28T09:50:09.408Z","end":"2024-01-28T09:50:09.514Z","steps":["trace[505934184] 'process raft request'  (duration: 105.735031ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T09:52:37.788Z","caller":"traceutil/trace.go:171","msg":"trace[1679279503] transaction","detail":"{read_only:false; response_revision:4155; number_of_response:1; }","duration":"354.324588ms","start":"2024-01-28T09:52:37.434Z","end":"2024-01-28T09:52:37.788Z","steps":["trace[1679279503] 'process raft request'  (duration: 353.954588ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T09:52:37.789Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-28T09:52:37.434Z","time spent":"354.613727ms","remote":"127.0.0.1:54502","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4153 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-01-28T09:52:41.509Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3920}
{"level":"info","ts":"2024-01-28T09:52:41.512Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3920,"took":"2.955288ms","hash":3683949235}
{"level":"info","ts":"2024-01-28T09:52:41.513Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3683949235,"revision":3920,"compact-revision":3636}

* 
* ==> etcd [bac8d43a354d] <==
* {"level":"info","ts":"2024-01-26T20:24:28.231Z","caller":"traceutil/trace.go:171","msg":"trace[2116948439] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3221; }","duration":"415.637181ms","start":"2024-01-26T20:24:27.816Z","end":"2024-01-26T20:24:28.231Z","steps":["trace[2116948439] 'range keys from in-memory index tree'  (duration: 413.022175ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:29.402Z","caller":"traceutil/trace.go:171","msg":"trace[427252981] transaction","detail":"{read_only:false; response_revision:3222; number_of_response:1; }","duration":"130.357628ms","start":"2024-01-26T20:24:29.271Z","end":"2024-01-26T20:24:29.402Z","steps":["trace[427252981] 'process raft request'  (duration: 63.344638ms)","trace[427252981] 'compare'  (duration: 66.500748ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:24:29.667Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"143.284363ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-26T20:24:29.667Z","caller":"traceutil/trace.go:171","msg":"trace[2132885370] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3222; }","duration":"143.496898ms","start":"2024-01-26T20:24:29.524Z","end":"2024-01-26T20:24:29.667Z","steps":["trace[2132885370] 'range keys from in-memory index tree'  (duration: 142.847959ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:34.685Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"547.050654ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751619194186058 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/two-tier-app-57f45bf559-tr8sv.17adff6c2d36028e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/two-tier-app-57f45bf559-tr8sv.17adff6c2d36028e\" value_size:659 lease:6571751619194185753 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-01-26T20:24:34.685Z","caller":"traceutil/trace.go:171","msg":"trace[1693019025] transaction","detail":"{read_only:false; response_revision:3227; number_of_response:1; }","duration":"758.634328ms","start":"2024-01-26T20:24:33.927Z","end":"2024-01-26T20:24:34.685Z","steps":["trace[1693019025] 'process raft request'  (duration: 210.907526ms)","trace[1693019025] 'compare'  (duration: 546.819557ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:24:34.686Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-26T20:24:33.927Z","time spent":"758.941151ms","remote":"127.0.0.1:34452","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":748,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/two-tier-app-57f45bf559-tr8sv.17adff6c2d36028e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/two-tier-app-57f45bf559-tr8sv.17adff6c2d36028e\" value_size:659 lease:6571751619194185753 >> failure:<>"}
{"level":"info","ts":"2024-01-26T20:24:34.686Z","caller":"traceutil/trace.go:171","msg":"trace[668314200] linearizableReadLoop","detail":"{readStateIndex:3930; appliedIndex:3928; }","duration":"159.836386ms","start":"2024-01-26T20:24:34.526Z","end":"2024-01-26T20:24:34.685Z","steps":["trace[668314200] 'read index received'  (duration: 317.183µs)","trace[668314200] 'applied index is now lower than readState.Index'  (duration: 159.516459ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:24:34.686Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.221199ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-26T20:24:34.687Z","caller":"traceutil/trace.go:171","msg":"trace[1689170943] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3227; }","duration":"160.876772ms","start":"2024-01-26T20:24:34.526Z","end":"2024-01-26T20:24:34.686Z","steps":["trace[1689170943] 'agreement among raft nodes before linearized reading'  (duration: 160.028178ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:36.533Z","caller":"traceutil/trace.go:171","msg":"trace[1008159706] transaction","detail":"{read_only:false; response_revision:3230; number_of_response:1; }","duration":"128.353331ms","start":"2024-01-26T20:24:36.405Z","end":"2024-01-26T20:24:36.533Z","steps":["trace[1008159706] 'process raft request'  (duration: 127.389309ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:37.714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.883512ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/two-tier-app-57f45bf559-tr8sv\" ","response":"range_response_count:1 size:2965"}
{"level":"info","ts":"2024-01-26T20:24:37.714Z","caller":"traceutil/trace.go:171","msg":"trace[1507423287] range","detail":"{range_begin:/registry/pods/default/two-tier-app-57f45bf559-tr8sv; range_end:; response_count:1; response_revision:3232; }","duration":"111.139646ms","start":"2024-01-26T20:24:37.603Z","end":"2024-01-26T20:24:37.714Z","steps":["trace[1507423287] 'range keys from in-memory index tree'  (duration: 110.679792ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:37.714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.75464ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-26T20:24:37.715Z","caller":"traceutil/trace.go:171","msg":"trace[2069813925] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3232; }","duration":"193.594536ms","start":"2024-01-26T20:24:37.521Z","end":"2024-01-26T20:24:37.715Z","steps":["trace[2069813925] 'range keys from in-memory index tree'  (duration: 192.403717ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:37.897Z","caller":"traceutil/trace.go:171","msg":"trace[2105095949] transaction","detail":"{read_only:false; response_revision:3233; number_of_response:1; }","duration":"162.732468ms","start":"2024-01-26T20:24:37.734Z","end":"2024-01-26T20:24:37.897Z","steps":["trace[2105095949] 'process raft request'  (duration: 162.359713ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:38.122Z","caller":"traceutil/trace.go:171","msg":"trace[162871634] transaction","detail":"{read_only:false; response_revision:3234; number_of_response:1; }","duration":"208.602344ms","start":"2024-01-26T20:24:37.913Z","end":"2024-01-26T20:24:38.121Z","steps":["trace[162871634] 'process raft request'  (duration: 109.368842ms)","trace[162871634] 'compare'  (duration: 98.756244ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:38.133Z","caller":"traceutil/trace.go:171","msg":"trace[511494197] transaction","detail":"{read_only:false; response_revision:3235; number_of_response:1; }","duration":"215.435564ms","start":"2024-01-26T20:24:37.918Z","end":"2024-01-26T20:24:38.133Z","steps":["trace[511494197] 'process raft request'  (duration: 214.311837ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:38.133Z","caller":"traceutil/trace.go:171","msg":"trace[566664713] transaction","detail":"{read_only:false; response_revision:3236; number_of_response:1; }","duration":"214.529995ms","start":"2024-01-26T20:24:37.919Z","end":"2024-01-26T20:24:38.133Z","steps":["trace[566664713] 'process raft request'  (duration: 213.617375ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:38.354Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.483766ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751619194186079 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/deployments/default/two-tier-app\" mod_revision:3219 > success:<request_put:<key:\"/registry/deployments/default/two-tier-app\" value_size:3234 >> failure:<request_range:<key:\"/registry/deployments/default/two-tier-app\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-26T20:24:38.354Z","caller":"traceutil/trace.go:171","msg":"trace[1652831230] transaction","detail":"{read_only:false; response_revision:3238; number_of_response:1; }","duration":"202.304433ms","start":"2024-01-26T20:24:38.152Z","end":"2024-01-26T20:24:38.354Z","steps":["trace[1652831230] 'process raft request'  (duration: 91.447045ms)","trace[1652831230] 'compare'  (duration: 110.33224ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:39.422Z","caller":"traceutil/trace.go:171","msg":"trace[341395647] transaction","detail":"{read_only:false; response_revision:3239; number_of_response:1; }","duration":"109.441285ms","start":"2024-01-26T20:24:39.312Z","end":"2024-01-26T20:24:39.422Z","steps":["trace[341395647] 'process raft request'  (duration: 52.362855ms)","trace[341395647] 'compare'  (duration: 56.816942ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:44.435Z","caller":"traceutil/trace.go:171","msg":"trace[667670043] transaction","detail":"{read_only:false; response_revision:3242; number_of_response:1; }","duration":"102.571394ms","start":"2024-01-26T20:24:44.332Z","end":"2024-01-26T20:24:44.435Z","steps":["trace[667670043] 'process raft request'  (duration: 102.302673ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:44.682Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.728123ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751619194186103 > lease_revoke:<id:5b338d476e2ae935>","response":"size:29"}
{"level":"info","ts":"2024-01-26T20:24:44.682Z","caller":"traceutil/trace.go:171","msg":"trace[1892414537] linearizableReadLoop","detail":"{readStateIndex:3947; appliedIndex:3946; }","duration":"154.058163ms","start":"2024-01-26T20:24:44.528Z","end":"2024-01-26T20:24:44.682Z","steps":["trace[1892414537] 'read index received'  (duration: 92.286µs)","trace[1892414537] 'applied index is now lower than readState.Index'  (duration: 153.963375ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:24:44.682Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"154.257845ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-26T20:24:44.682Z","caller":"traceutil/trace.go:171","msg":"trace[1452070824] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3242; }","duration":"154.31959ms","start":"2024-01-26T20:24:44.528Z","end":"2024-01-26T20:24:44.682Z","steps":["trace[1452070824] 'agreement among raft nodes before linearized reading'  (duration: 154.166147ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:46.686Z","caller":"traceutil/trace.go:171","msg":"trace[1867580441] transaction","detail":"{read_only:false; response_revision:3243; number_of_response:1; }","duration":"106.714504ms","start":"2024-01-26T20:24:46.580Z","end":"2024-01-26T20:24:46.686Z","steps":["trace[1867580441] 'process raft request'  (duration: 106.584709ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:46.854Z","caller":"traceutil/trace.go:171","msg":"trace[156822811] linearizableReadLoop","detail":"{readStateIndex:3949; appliedIndex:3948; }","duration":"164.906272ms","start":"2024-01-26T20:24:46.689Z","end":"2024-01-26T20:24:46.854Z","steps":["trace[156822811] 'read index received'  (duration: 75.059921ms)","trace[156822811] 'applied index is now lower than readState.Index'  (duration: 89.842788ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:46.855Z","caller":"traceutil/trace.go:171","msg":"trace[954531925] transaction","detail":"{read_only:false; response_revision:3244; number_of_response:1; }","duration":"168.001106ms","start":"2024-01-26T20:24:46.687Z","end":"2024-01-26T20:24:46.855Z","steps":["trace[954531925] 'process raft request'  (duration: 77.456965ms)","trace[954531925] 'compare'  (duration: 89.577769ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:24:46.889Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.635536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/\" range_end:\"/registry/services/specs/default0\" limit:500 ","response":"range_response_count:3 size:2662"}
{"level":"info","ts":"2024-01-26T20:24:46.889Z","caller":"traceutil/trace.go:171","msg":"trace[89681938] range","detail":"{range_begin:/registry/services/specs/default/; range_end:/registry/services/specs/default0; response_count:3; response_revision:3244; }","duration":"199.827879ms","start":"2024-01-26T20:24:46.689Z","end":"2024-01-26T20:24:46.889Z","steps":["trace[89681938] 'agreement among raft nodes before linearized reading'  (duration: 165.119129ms)","trace[89681938] 'range keys from bolt db'  (duration: 34.249804ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:48.910Z","caller":"traceutil/trace.go:171","msg":"trace[777647659] transaction","detail":"{read_only:false; response_revision:3246; number_of_response:1; }","duration":"168.054663ms","start":"2024-01-26T20:24:48.742Z","end":"2024-01-26T20:24:48.910Z","steps":["trace[777647659] 'process raft request'  (duration: 166.919466ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:24:49.454Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.797663ms","expected-duration":"100ms","prefix":"","request":"header:<ID:6571751619194186123 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.94.2\" mod_revision:3239 > success:<request_put:<key:\"/registry/masterleases/192.168.94.2\" value_size:65 lease:6571751619194186121 >> failure:<request_range:<key:\"/registry/masterleases/192.168.94.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-26T20:24:49.455Z","caller":"traceutil/trace.go:171","msg":"trace[735311571] transaction","detail":"{read_only:false; response_revision:3247; number_of_response:1; }","duration":"178.940246ms","start":"2024-01-26T20:24:49.275Z","end":"2024-01-26T20:24:49.454Z","steps":["trace[735311571] 'process raft request'  (duration: 77.674346ms)","trace[735311571] 'compare'  (duration: 100.501257ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:24:53.120Z","caller":"traceutil/trace.go:171","msg":"trace[122980723] transaction","detail":"{read_only:false; response_revision:3249; number_of_response:1; }","duration":"135.298836ms","start":"2024-01-26T20:24:52.985Z","end":"2024-01-26T20:24:53.120Z","steps":["trace[122980723] 'process raft request'  (duration: 135.17503ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:24:59.389Z","caller":"traceutil/trace.go:171","msg":"trace[1709354474] transaction","detail":"{read_only:false; response_revision:3254; number_of_response:1; }","duration":"130.23861ms","start":"2024-01-26T20:24:59.259Z","end":"2024-01-26T20:24:59.389Z","steps":["trace[1709354474] 'process raft request'  (duration: 98.972076ms)","trace[1709354474] 'compare'  (duration: 30.82163ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:25:09.430Z","caller":"traceutil/trace.go:171","msg":"trace[9763680] transaction","detail":"{read_only:false; response_revision:3262; number_of_response:1; }","duration":"129.803524ms","start":"2024-01-26T20:25:09.300Z","end":"2024-01-26T20:25:09.430Z","steps":["trace[9763680] 'process raft request'  (duration: 47.875663ms)","trace[9763680] 'compare'  (duration: 81.615754ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:26:59.859Z","caller":"traceutil/trace.go:171","msg":"trace[271188346] transaction","detail":"{read_only:false; response_revision:3362; number_of_response:1; }","duration":"108.813344ms","start":"2024-01-26T20:26:59.731Z","end":"2024-01-26T20:26:59.840Z","steps":["trace[271188346] 'process raft request'  (duration: 108.643767ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:07.997Z","caller":"traceutil/trace.go:171","msg":"trace[1348850204] transaction","detail":"{read_only:false; response_revision:3373; number_of_response:1; }","duration":"179.570354ms","start":"2024-01-26T20:27:07.818Z","end":"2024-01-26T20:27:07.997Z","steps":["trace[1348850204] 'process raft request'  (duration: 179.211912ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:07.999Z","caller":"traceutil/trace.go:171","msg":"trace[1971298718] linearizableReadLoop","detail":"{readStateIndex:4109; appliedIndex:4107; }","duration":"153.920553ms","start":"2024-01-26T20:27:07.843Z","end":"2024-01-26T20:27:07.997Z","steps":["trace[1971298718] 'read index received'  (duration: 78.41593ms)","trace[1971298718] 'applied index is now lower than readState.Index'  (duration: 75.50097ms)"],"step_count":2}
{"level":"info","ts":"2024-01-26T20:27:08.001Z","caller":"traceutil/trace.go:171","msg":"trace[1995853956] transaction","detail":"{read_only:false; number_of_response:1; response_revision:3372; }","duration":"180.104717ms","start":"2024-01-26T20:27:07.818Z","end":"2024-01-26T20:27:07.998Z","steps":["trace[1995853956] 'process raft request'  (duration: 103.918821ms)","trace[1995853956] 'compare'  (duration: 75.090183ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:27:08.011Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"167.884206ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/pv-protection-controller\" ","response":"range_response_count:1 size:214"}
{"level":"info","ts":"2024-01-26T20:27:08.011Z","caller":"traceutil/trace.go:171","msg":"trace[1354624739] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/pv-protection-controller; range_end:; response_count:1; response_revision:3373; }","duration":"168.091969ms","start":"2024-01-26T20:27:07.843Z","end":"2024-01-26T20:27:08.011Z","steps":["trace[1354624739] 'agreement among raft nodes before linearized reading'  (duration: 155.492887ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:08.152Z","caller":"traceutil/trace.go:171","msg":"trace[951115238] transaction","detail":"{read_only:false; response_revision:3375; number_of_response:1; }","duration":"133.588218ms","start":"2024-01-26T20:27:08.018Z","end":"2024-01-26T20:27:08.152Z","steps":["trace[951115238] 'process raft request'  (duration: 80.759437ms)","trace[951115238] 'compare'  (duration: 52.670275ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-26T20:27:08.152Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.835446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/mysql\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-01-26T20:27:08.152Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.439942ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/mysql-kv7m5\" ","response":"range_response_count:1 size:1068"}
{"level":"info","ts":"2024-01-26T20:27:08.152Z","caller":"traceutil/trace.go:171","msg":"trace[1663141797] range","detail":"{range_begin:/registry/services/endpoints/default/mysql; range_end:; response_count:0; response_revision:3375; }","duration":"133.892401ms","start":"2024-01-26T20:27:08.018Z","end":"2024-01-26T20:27:08.152Z","steps":["trace[1663141797] 'agreement among raft nodes before linearized reading'  (duration: 133.744316ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:08.152Z","caller":"traceutil/trace.go:171","msg":"trace[1265233604] range","detail":"{range_begin:/registry/endpointslices/default/mysql-kv7m5; range_end:; response_count:1; response_revision:3375; }","duration":"133.507213ms","start":"2024-01-26T20:27:08.019Z","end":"2024-01-26T20:27:08.152Z","steps":["trace[1265233604] 'agreement among raft nodes before linearized reading'  (duration: 133.321171ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:27:08.152Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.932139ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/pv-protection-controller\" ","response":"range_response_count:1 size:214"}
{"level":"info","ts":"2024-01-26T20:27:08.152Z","caller":"traceutil/trace.go:171","msg":"trace[967418319] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/pv-protection-controller; range_end:; response_count:1; response_revision:3375; }","duration":"129.977543ms","start":"2024-01-26T20:27:08.022Z","end":"2024-01-26T20:27:08.152Z","steps":["trace[967418319] 'agreement among raft nodes before linearized reading'  (duration: 129.478819ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-26T20:27:08.511Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.811623ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-26T20:27:08.511Z","caller":"traceutil/trace.go:171","msg":"trace[333968508] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:3378; }","duration":"129.007385ms","start":"2024-01-26T20:27:08.382Z","end":"2024-01-26T20:27:08.511Z","steps":["trace[333968508] 'count revisions from in-memory index tree'  (duration: 128.479727ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:11.312Z","caller":"traceutil/trace.go:171","msg":"trace[335630646] transaction","detail":"{read_only:false; response_revision:3382; number_of_response:1; }","duration":"141.089098ms","start":"2024-01-26T20:27:11.171Z","end":"2024-01-26T20:27:11.312Z","steps":["trace[335630646] 'process raft request'  (duration: 140.76528ms)"],"step_count":1}
{"level":"info","ts":"2024-01-26T20:27:18.509Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-26T20:27:18.509Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.94.2:2380"],"advertise-client-urls":["https://192.168.94.2:2379"]}
{"level":"info","ts":"2024-01-26T20:27:18.750Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"dfc97eb0aae75b33","current-leader-member-id":"dfc97eb0aae75b33"}
{"level":"info","ts":"2024-01-26T20:27:18.909Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.94.2:2380"}
{"level":"info","ts":"2024-01-26T20:27:18.910Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.94.2:2380"}
{"level":"info","ts":"2024-01-26T20:27:18.910Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.94.2:2380"],"advertise-client-urls":["https://192.168.94.2:2379"]}

* 
* ==> kernel <==
*  09:52:57 up 15:12,  0 users,  load average: 0.64, 1.97, 1.68
Linux minikube 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [126cb3c946e7] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.317743       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.329205       1 logging.go:59] [core] [Channel #33 SubChannel #36] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.356363       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.369098       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.370538       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.464379       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0126 20:27:28.478115       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [ab59dee2daf1] <==
* Trace[1430394476]: [631.905104ms] [631.905104ms] END
I0128 09:42:53.111420       1 trace.go:219] Trace[2072391198]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8b3ed647-c677-438f-992e-493582838c3b,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (28-Jan-2024 09:42:52.568) (total time: 542ms):
Trace[2072391198]: ["GuaranteedUpdate etcd3" audit-id:8b3ed647-c677-438f-992e-493582838c3b,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 542ms (09:42:52.568)
Trace[2072391198]:  ---"Txn call completed" 541ms (09:42:53.111)]
Trace[2072391198]: [542.676378ms] [542.676378ms] END
I0128 09:46:13.461146       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0128 09:46:14.164065       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs=map[IPv4:10.100.111.186]
I0128 09:46:14.164321       1 trace.go:219] Trace[1420042732]: "Create" accept:application/json,audit-id:57e76f80-7a8c-4b0b-b996-2bd62ab086d0,client:192.168.94.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services,user-agent:kubectl/v1.29.1 (linux/amd64) kubernetes/bc401b9,verb:POST (28-Jan-2024 09:46:13.575) (total time: 588ms):
Trace[1420042732]: ---"Write to database call succeeded" len:452 117ms (09:46:14.164)
Trace[1420042732]: [588.285989ms] [588.285989ms] END
I0128 09:47:26.187101       1 trace.go:219] Trace[647797227]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a4b8dbc7-0597-45c1-92be-ec08d78011dc,client:192.168.94.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/system:serviceaccount:kube-system:replicaset-controller,verb:POST (28-Jan-2024 09:47:25.615) (total time: 571ms):
Trace[647797227]: [571.045138ms] [571.045138ms] END
I0128 09:47:26.187455       1 alloc.go:330] "allocated clusterIPs" service="default/two-tier-app-service" clusterIPs=map[IPv4:10.97.41.39]
I0128 09:47:26.187974       1 trace.go:219] Trace[1746341904]: "Create" accept:application/json,audit-id:66e817c6-18ef-48a7-8be5-0ea89d3c02be,client:192.168.94.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services,user-agent:kubectl/v1.29.1 (linux/amd64) kubernetes/bc401b9,verb:POST (28-Jan-2024 09:47:25.442) (total time: 745ms):
Trace[1746341904]: [745.057366ms] [745.057366ms] END
I0128 09:47:49.308681       1 trace.go:219] Trace[896584915]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.94.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 09:47:48.594) (total time: 714ms):
Trace[896584915]: ---"Transaction prepared" 361ms (09:47:48.957)
Trace[896584915]: ---"Txn call completed" 350ms (09:47:49.308)
Trace[896584915]: [714.341948ms] [714.341948ms] END
I0128 09:47:50.616752       1 trace.go:219] Trace[1159746206]: "Update" accept:application/json, */*,audit-id:31d15ff6-46fd-451a-a6a8-0a2f4e8e46d4,client:192.168.94.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 09:47:49.732) (total time: 884ms):
Trace[1159746206]: ["GuaranteedUpdate etcd3" audit-id:31d15ff6-46fd-451a-a6a8-0a2f4e8e46d4,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 883ms (09:47:49.732)
Trace[1159746206]:  ---"Txn call completed" 882ms (09:47:50.616)]
Trace[1159746206]: [884.413134ms] [884.413134ms] END
I0128 09:48:01.867559       1 trace.go:219] Trace[1955371816]: "Update" accept:application/json, */*,audit-id:0898d394-a621-4471-9fc4-1f98541121ea,client:192.168.94.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 09:48:01.342) (total time: 525ms):
Trace[1955371816]: ["GuaranteedUpdate etcd3" audit-id:0898d394-a621-4471-9fc4-1f98541121ea,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 524ms (09:48:01.342)
Trace[1955371816]:  ---"Txn call completed" 523ms (09:48:01.867)]
Trace[1955371816]: [525.197532ms] [525.197532ms] END
I0128 09:48:07.147686       1 trace.go:219] Trace[1047878750]: "Update" accept:application/json, */*,audit-id:b495ec97-3fcb-40ce-ae1d-80dfe551ee27,client:192.168.94.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 09:48:06.335) (total time: 812ms):
Trace[1047878750]: ---"limitedReadBody succeeded" len:1353 82ms (09:48:06.417)
Trace[1047878750]: ["GuaranteedUpdate etcd3" audit-id:b495ec97-3fcb-40ce-ae1d-80dfe551ee27,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 729ms (09:48:06.417)
Trace[1047878750]:  ---"Txn call completed" 728ms (09:48:07.147)]
Trace[1047878750]: [812.451509ms] [812.451509ms] END
I0128 09:48:09.174844       1 trace.go:219] Trace[845237516]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.94.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 09:48:08.582) (total time: 592ms):
Trace[845237516]: ---"Transaction prepared" 154ms (09:48:08.738)
Trace[845237516]: ---"Txn call completed" 436ms (09:48:09.174)
Trace[845237516]: [592.086013ms] [592.086013ms] END
I0128 09:48:09.718965       1 trace.go:219] Trace[434322659]: "Update" accept:application/json, */*,audit-id:bf6e03d7-3046-4b06-b637-242d513d1374,client:192.168.94.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 09:48:09.179) (total time: 539ms):
Trace[434322659]: ["GuaranteedUpdate etcd3" audit-id:bf6e03d7-3046-4b06-b637-242d513d1374,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 538ms (09:48:09.180)
Trace[434322659]:  ---"Txn call completed" 534ms (09:48:09.718)]
Trace[434322659]: [539.305677ms] [539.305677ms] END
I0128 09:48:09.719187       1 trace.go:219] Trace[237154272]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cc746baa-2e71-4d98-8b4c-8259eef8796d,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (28-Jan-2024 09:48:09.185) (total time: 533ms):
Trace[237154272]: ---"About to write a response" 533ms (09:48:09.718)
Trace[237154272]: [533.766543ms] [533.766543ms] END
I0128 09:48:19.648599       1 trace.go:219] Trace[693752548]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.94.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 09:48:19.051) (total time: 596ms):
Trace[693752548]: ---"Transaction prepared" 149ms (09:48:19.204)
Trace[693752548]: ---"Txn call completed" 443ms (09:48:19.648)
Trace[693752548]: [596.484881ms] [596.484881ms] END
I0128 09:48:19.651880       1 trace.go:219] Trace[1797595032]: "Update" accept:application/json, */*,audit-id:5b5e190c-6a41-439f-bbf7-20038e591e60,client:192.168.94.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 09:48:19.054) (total time: 597ms):
Trace[1797595032]: ["GuaranteedUpdate etcd3" audit-id:5b5e190c-6a41-439f-bbf7-20038e591e60,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 597ms (09:48:19.054)
Trace[1797595032]:  ---"Txn call completed" 594ms (09:48:19.651)]
Trace[1797595032]: [597.651708ms] [597.651708ms] END
I0128 09:48:33.395018       1 trace.go:219] Trace[231819418]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f9c10cf8-bc8b-40e1-92bc-7707e0f09f15,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (28-Jan-2024 09:48:32.230) (total time: 1164ms):
Trace[231819418]: ["GuaranteedUpdate etcd3" audit-id:f9c10cf8-bc8b-40e1-92bc-7707e0f09f15,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1164ms (09:48:32.230)
Trace[231819418]:  ---"Txn call completed" 1163ms (09:48:33.394)]
Trace[231819418]: [1.164647967s] [1.164647967s] END
I0128 09:48:39.851868       1 trace.go:219] Trace[1451911451]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:517c0e0c-d727-4d04-8364-87f75ae69c44,client:192.168.94.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PATCH (28-Jan-2024 09:48:39.162) (total time: 689ms):
Trace[1451911451]: ["GuaranteedUpdate etcd3" audit-id:517c0e0c-d727-4d04-8364-87f75ae69c44,key:/minions/minikube,type:*core.Node,resource:nodes 689ms (09:48:39.162)
Trace[1451911451]:  ---"Txn call completed" 684ms (09:48:39.850)]
Trace[1451911451]: ---"Object stored in database" 685ms (09:48:39.851)
Trace[1451911451]: [689.30839ms] [689.30839ms] END

* 
* ==> kube-controller-manager [4fcc6f93e096] <==
* I0128 09:38:00.182884       1 namespace_controller.go:197] "Starting namespace controller"
I0128 09:38:00.182935       1 shared_informer.go:311] Waiting for caches to sync for namespace
I0128 09:38:00.206709       1 controllermanager.go:638] "Started controller" controller="serviceaccount"
I0128 09:38:00.209278       1 serviceaccounts_controller.go:111] "Starting service account controller"
I0128 09:38:00.210042       1 shared_informer.go:311] Waiting for caches to sync for service account
I0128 09:38:00.234164       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0128 09:38:00.317966       1 shared_informer.go:318] Caches are synced for crt configmap
I0128 09:38:00.346658       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0128 09:38:00.347676       1 shared_informer.go:318] Caches are synced for PV protection
I0128 09:38:00.350718       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0128 09:38:00.353041       1 shared_informer.go:318] Caches are synced for TTL after finished
I0128 09:38:00.396915       1 shared_informer.go:318] Caches are synced for node
I0128 09:38:00.397181       1 range_allocator.go:174] "Sending events to api server"
I0128 09:38:00.397330       1 range_allocator.go:178] "Starting range CIDR allocator"
I0128 09:38:00.397400       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0128 09:38:00.397453       1 shared_informer.go:318] Caches are synced for cidrallocator
I0128 09:38:00.397684       1 shared_informer.go:318] Caches are synced for cronjob
I0128 09:38:00.398008       1 shared_informer.go:318] Caches are synced for TTL
I0128 09:38:00.398138       1 shared_informer.go:318] Caches are synced for namespace
I0128 09:38:00.396956       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0128 09:38:00.399320       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0128 09:38:00.399403       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0128 09:38:00.400005       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 09:38:00.403127       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 09:38:00.408918       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 09:38:00.412669       1 shared_informer.go:318] Caches are synced for service account
I0128 09:38:00.431227       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0128 09:38:00.433636       1 shared_informer.go:318] Caches are synced for attach detach
I0128 09:38:00.433731       1 shared_informer.go:318] Caches are synced for GC
I0128 09:38:00.434996       1 shared_informer.go:318] Caches are synced for resource quota
I0128 09:38:00.440238       1 shared_informer.go:318] Caches are synced for endpoint
I0128 09:38:00.446679       1 shared_informer.go:318] Caches are synced for deployment
I0128 09:38:00.485461       1 shared_informer.go:318] Caches are synced for HPA
I0128 09:38:00.485616       1 shared_informer.go:318] Caches are synced for stateful set
I0128 09:38:00.486149       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0128 09:38:00.489696       1 shared_informer.go:318] Caches are synced for disruption
I0128 09:38:00.485458       1 shared_informer.go:318] Caches are synced for expand
I0128 09:38:00.492074       1 shared_informer.go:318] Caches are synced for daemon sets
I0128 09:38:00.492787       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0128 09:38:00.563267       1 shared_informer.go:318] Caches are synced for taint
I0128 09:38:00.563794       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0128 09:38:00.564260       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0128 09:38:00.564438       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0128 09:38:00.564533       1 shared_informer.go:318] Caches are synced for job
I0128 09:38:00.564684       1 shared_informer.go:318] Caches are synced for ReplicationController
I0128 09:38:00.564823       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0128 09:38:00.564930       1 taint_manager.go:211] "Sending events to api server"
I0128 09:38:00.565765       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0128 09:38:00.575155       1 shared_informer.go:318] Caches are synced for persistent volume
I0128 09:38:00.575655       1 shared_informer.go:318] Caches are synced for ephemeral
I0128 09:38:00.591024       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0128 09:38:00.591025       1 shared_informer.go:318] Caches are synced for PVC protection
I0128 09:38:00.592577       1 shared_informer.go:318] Caches are synced for resource quota
I0128 09:38:00.851217       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 09:38:00.893134       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 09:38:00.893191       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0128 09:46:13.573922       1 event.go:307] "Event occurred" object="default/mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-6dd4c8b9c4 to 1"
I0128 09:46:13.913107       1 event.go:307] "Event occurred" object="default/mysql-6dd4c8b9c4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-6dd4c8b9c4-pg8h9"
I0128 09:47:25.607513       1 event.go:307] "Event occurred" object="default/two-tier-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set two-tier-app-5467879f76 to 1"
I0128 09:47:26.197840       1 event.go:307] "Event occurred" object="default/two-tier-app-5467879f76" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: two-tier-app-5467879f76-wsm25"

* 
* ==> kube-controller-manager [674ed394a7a3] <==
* I0126 20:21:23.158607       1 controllermanager.go:638] "Started controller" controller="nodeipam"
I0126 20:21:23.158854       1 node_ipam_controller.go:162] "Starting ipam controller"
I0126 20:21:23.158879       1 shared_informer.go:311] Waiting for caches to sync for node
I0126 20:21:23.179370       1 shared_informer.go:318] Caches are synced for PV protection
I0126 20:21:23.186330       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0126 20:21:23.207618       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0126 20:21:23.217136       1 shared_informer.go:318] Caches are synced for ephemeral
I0126 20:21:23.218603       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0126 20:21:23.261475       1 shared_informer.go:318] Caches are synced for GC
I0126 20:21:23.262187       1 shared_informer.go:318] Caches are synced for endpoint
I0126 20:21:23.262255       1 shared_informer.go:318] Caches are synced for crt configmap
I0126 20:21:23.262437       1 shared_informer.go:318] Caches are synced for persistent volume
I0126 20:21:23.262668       1 shared_informer.go:318] Caches are synced for daemon sets
I0126 20:21:23.268214       1 shared_informer.go:318] Caches are synced for node
I0126 20:21:23.268341       1 range_allocator.go:174] "Sending events to api server"
I0126 20:21:23.268445       1 range_allocator.go:178] "Starting range CIDR allocator"
I0126 20:21:23.268468       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0126 20:21:23.268483       1 shared_informer.go:318] Caches are synced for cidrallocator
I0126 20:21:23.268611       1 shared_informer.go:318] Caches are synced for TTL
I0126 20:21:23.268213       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0126 20:21:23.319800       1 shared_informer.go:318] Caches are synced for stateful set
I0126 20:21:23.319945       1 shared_informer.go:318] Caches are synced for deployment
I0126 20:21:23.320007       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0126 20:21:23.331503       1 shared_informer.go:318] Caches are synced for job
I0126 20:21:23.331861       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0126 20:21:23.332151       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0126 20:21:23.336595       1 shared_informer.go:318] Caches are synced for expand
I0126 20:21:23.336763       1 shared_informer.go:318] Caches are synced for PVC protection
I0126 20:21:23.336901       1 shared_informer.go:318] Caches are synced for service account
I0126 20:21:23.337089       1 shared_informer.go:318] Caches are synced for disruption
I0126 20:21:23.342207       1 shared_informer.go:318] Caches are synced for HPA
I0126 20:21:23.343308       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0126 20:21:23.343802       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0126 20:21:23.348189       1 shared_informer.go:318] Caches are synced for TTL after finished
I0126 20:21:23.348816       1 shared_informer.go:318] Caches are synced for ReplicationController
I0126 20:21:23.353147       1 shared_informer.go:318] Caches are synced for namespace
I0126 20:21:23.353189       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0126 20:21:23.353220       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0126 20:21:23.353274       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0126 20:21:23.353319       1 shared_informer.go:318] Caches are synced for attach detach
I0126 20:21:23.357953       1 shared_informer.go:318] Caches are synced for taint
I0126 20:21:23.358412       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0126 20:21:23.359219       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0126 20:21:23.359348       1 taint_manager.go:211] "Sending events to api server"
I0126 20:21:23.359788       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0126 20:21:23.360216       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0126 20:21:23.363501       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0126 20:21:23.412025       1 shared_informer.go:318] Caches are synced for resource quota
I0126 20:21:23.419466       1 shared_informer.go:318] Caches are synced for cronjob
I0126 20:21:23.432870       1 shared_informer.go:318] Caches are synced for resource quota
I0126 20:21:23.823444       1 shared_informer.go:318] Caches are synced for garbage collector
I0126 20:21:23.823534       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0126 20:21:23.844289       1 shared_informer.go:318] Caches are synced for garbage collector
I0126 20:22:37.245970       1 event.go:307] "Event occurred" object="default/mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-7d94d9bf46 to 1"
I0126 20:22:37.629725       1 event.go:307] "Event occurred" object="default/mysql-pvc" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0126 20:22:37.630480       1 event.go:307] "Event occurred" object="default/mysql-7d94d9bf46" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-7d94d9bf46-tfsqm"
I0126 20:23:32.495505       1 event.go:307] "Event occurred" object="default/two-tier-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set two-tier-app-5b6b79789c to 1"
I0126 20:23:33.143501       1 event.go:307] "Event occurred" object="default/two-tier-app-5b6b79789c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: two-tier-app-5b6b79789c-tmmph"
I0126 20:24:26.280861       1 event.go:307] "Event occurred" object="default/two-tier-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set two-tier-app-57f45bf559 to 1"
I0126 20:24:26.595545       1 event.go:307] "Event occurred" object="default/two-tier-app-57f45bf559" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: two-tier-app-57f45bf559-tr8sv"

* 
* ==> kube-proxy [d816df1e7fe4] <==
* I0128 09:38:34.722236       1 node.go:141] Successfully retrieved node IP: 192.168.94.2
I0128 09:38:34.722493       1 server_others.go:110] "Detected node IP" address="192.168.94.2"
I0128 09:38:34.722623       1 server_others.go:554] "Using iptables proxy"
I0128 09:38:36.121994       1 server_others.go:192] "Using iptables Proxier"
I0128 09:38:36.122065       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0128 09:38:36.122091       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0128 09:38:36.122122       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0128 09:38:36.122266       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0128 09:38:36.123889       1 server.go:658] "Version info" version="v1.27.4"
I0128 09:38:36.124037       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 09:38:36.228333       1 config.go:97] "Starting endpoint slice config controller"
I0128 09:38:36.228419       1 config.go:315] "Starting node config controller"
I0128 09:38:36.228437       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0128 09:38:36.228457       1 shared_informer.go:311] Waiting for caches to sync for node config
I0128 09:38:36.228332       1 config.go:188] "Starting service config controller"
I0128 09:38:36.228553       1 shared_informer.go:311] Waiting for caches to sync for service config
I0128 09:38:36.328814       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0128 09:38:36.328832       1 shared_informer.go:318] Caches are synced for node config
I0128 09:38:36.328997       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [e2af9e012937] <==
* E0126 20:20:26.772003       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:27.792507       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:29.799609       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.94.2:8443: connect: connection refused
I0126 20:20:39.982778       1 node.go:141] Successfully retrieved node IP: 192.168.94.2
I0126 20:20:39.982948       1 server_others.go:110] "Detected node IP" address="192.168.94.2"
I0126 20:20:39.983111       1 server_others.go:554] "Using iptables proxy"
I0126 20:20:41.536731       1 server_others.go:192] "Using iptables Proxier"
I0126 20:20:41.536781       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0126 20:20:41.536805       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0126 20:20:41.536877       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0126 20:20:41.629982       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0126 20:20:41.695827       1 server.go:658] "Version info" version="v1.27.4"
I0126 20:20:41.695870       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0126 20:20:41.876812       1 config.go:97] "Starting endpoint slice config controller"
I0126 20:20:42.019609       1 config.go:188] "Starting service config controller"
I0126 20:20:42.019712       1 shared_informer.go:311] Waiting for caches to sync for service config
I0126 20:20:42.019844       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0126 20:20:42.019677       1 config.go:315] "Starting node config controller"
I0126 20:20:42.020241       1 shared_informer.go:311] Waiting for caches to sync for node config
I0126 20:20:42.196390       1 shared_informer.go:318] Caches are synced for service config
I0126 20:20:42.219978       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0126 20:20:42.320503       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [7221579667df] <==
* E0128 09:37:36.713927       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.94.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:36.743857       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.94.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:36.743917       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.94.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:36.844592       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.94.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:36.844666       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.94.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.237594       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.94.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.237731       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.94.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.242629       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.242775       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.366762       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.94.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.366996       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.94.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.439914       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.94.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.440020       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.94.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.520261       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.520367       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.701998       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.94.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.702062       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.94.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.706526       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.706584       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:37.792265       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.94.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:37.792423       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.94.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:38.166770       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:38.166826       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:38.180086       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.94.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0128 09:37:38.180136       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.94.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0128 09:37:43.895029       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0128 09:37:43.895075       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0128 09:37:43.973034       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0128 09:37:43.973091       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0128 09:37:43.973290       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0128 09:37:43.973330       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0128 09:37:43.973442       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0128 09:37:43.973484       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0128 09:37:43.973591       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0128 09:37:43.973626       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0128 09:37:43.973725       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0128 09:37:43.973821       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0128 09:37:43.973989       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0128 09:37:43.974084       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0128 09:37:43.974253       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0128 09:37:43.974338       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0128 09:37:43.974643       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0128 09:37:43.974736       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0128 09:37:43.976877       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0128 09:37:43.977078       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0128 09:37:43.977179       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0128 09:37:43.977291       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0128 09:37:43.977547       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0128 09:37:43.977656       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0128 09:37:43.977896       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0128 09:37:43.978000       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0128 09:37:43.978261       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0128 09:37:43.978378       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0128 09:37:43.984852       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0128 09:37:43.992832       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
I0128 09:37:55.651106       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 09:46:14.362258       1 trace.go:219] Trace[1960487590]: "Scheduling" namespace:default,name:mysql-6dd4c8b9c4-pg8h9 (28-Jan-2024 09:46:13.907) (total time: 400ms):
Trace[1960487590]: ---"Snapshotting scheduler cache and node infos done" 52ms (09:46:13.959)
Trace[1960487590]: ---"Computing predicates done" 348ms (09:46:14.307)
Trace[1960487590]: [400.308896ms] [400.308896ms] END

* 
* ==> kube-scheduler [8a2d25261c21] <==
* W0126 20:20:30.646128       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.94.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.648803       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.649048       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.94.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.649059       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.94.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:30.648431       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.650375       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:30.650575       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.94.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.650950       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.94.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.651081       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.94.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:30.651811       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.275266       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.94.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.275435       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.94.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.339124       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.94.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.339218       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.94.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.474212       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.94.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.474300       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.94.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.522070       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.522146       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.562724       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.94.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.562811       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.94.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.656340       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.656447       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.94.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.670109       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.94.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.670190       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.94.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:31.687737       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
E0126 20:20:31.687797       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.94.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.94.2:8443: connect: connection refused
W0126 20:20:39.074783       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0126 20:20:39.075126       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0126 20:20:39.075643       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0126 20:20:39.075879       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0126 20:20:39.075899       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0126 20:20:39.076263       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0126 20:20:39.076093       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0126 20:20:39.076688       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0126 20:20:39.076789       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0126 20:20:39.077096       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0126 20:20:39.076630       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0126 20:20:39.077450       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0126 20:20:39.076229       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0126 20:20:39.077923       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0126 20:20:39.076912       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0126 20:20:39.078341       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0126 20:20:39.078934       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0126 20:20:39.079146       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0126 20:20:39.079353       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0126 20:20:39.079551       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0126 20:20:39.079883       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0126 20:20:39.080075       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0126 20:20:39.080128       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0126 20:20:39.080479       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0126 20:20:39.080235       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0126 20:20:39.080890       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0126 20:20:39.080023       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0126 20:20:39.081104       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0126 20:20:39.086662       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0126 20:20:39.086972       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0126 20:20:43.134533       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0126 20:27:18.455601       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E0126 20:27:18.455936       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E0126 20:27:18.456005       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Jan 28 09:37:52 minikube kubelet[1169]: E0128 09:37:52.066207    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-bvmwv" podUID=d7a13a82-e369-408e-ab10-398462ed2f28
Jan 28 09:37:53 minikube kubelet[1169]: I0128 09:37:53.075825    1169 scope.go:115] "RemoveContainer" containerID="592cb096c157b327da890c174e58418231056c514977d081b2312b2643b84f75"
Jan 28 09:37:53 minikube kubelet[1169]: I0128 09:37:53.075932    1169 scope.go:115] "RemoveContainer" containerID="e2af9e0129377ed3fccda513368e6b3df7fbcf0f7326554881489a95c20ed0b9"
Jan 28 09:37:53 minikube kubelet[1169]: I0128 09:37:53.076018    1169 scope.go:115] "RemoveContainer" containerID="f279f323a985d052c4f82bd44d20b4089c42c53fd8ed87d539d1612e7e5af8ae"
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.079072    1169 kuberuntime_manager.go:1212] container &Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-228fn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod coredns-5d78c9869d-wf96w_kube-system(53c4ae71-7ecb-462f-9619-a1376ccd498b): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.079167    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/coredns-5d78c9869d-wf96w" podUID=53c4ae71-7ecb-462f-9619-a1376ccd498b
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.081882    1169 kuberuntime_manager.go:1212] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vwcml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.081966    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.084537    1169 kuberuntime_manager.go:1212] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.4,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-dccth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod kube-proxy-bvmwv_kube-system(d7a13a82-e369-408e-ab10-398462ed2f28): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:37:53 minikube kubelet[1169]: E0128 09:37:53.084614    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-bvmwv" podUID=d7a13a82-e369-408e-ab10-398462ed2f28
Jan 28 09:38:05 minikube kubelet[1169]: I0128 09:38:05.782408    1169 scope.go:115] "RemoveContainer" containerID="592cb096c157b327da890c174e58418231056c514977d081b2312b2643b84f75"
Jan 28 09:38:05 minikube kubelet[1169]: E0128 09:38:05.789684    1169 kuberuntime_manager.go:1212] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vwcml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:38:05 minikube kubelet[1169]: E0128 09:38:05.789927    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:38:06 minikube kubelet[1169]: I0128 09:38:06.781800    1169 scope.go:115] "RemoveContainer" containerID="e2af9e0129377ed3fccda513368e6b3df7fbcf0f7326554881489a95c20ed0b9"
Jan 28 09:38:06 minikube kubelet[1169]: E0128 09:38:06.786930    1169 kuberuntime_manager.go:1212] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.4,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-dccth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod kube-proxy-bvmwv_kube-system(d7a13a82-e369-408e-ab10-398462ed2f28): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:38:06 minikube kubelet[1169]: E0128 09:38:06.787115    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-bvmwv" podUID=d7a13a82-e369-408e-ab10-398462ed2f28
Jan 28 09:38:07 minikube kubelet[1169]: I0128 09:38:07.781272    1169 scope.go:115] "RemoveContainer" containerID="f279f323a985d052c4f82bd44d20b4089c42c53fd8ed87d539d1612e7e5af8ae"
Jan 28 09:38:07 minikube kubelet[1169]: E0128 09:38:07.783675    1169 kuberuntime_manager.go:1212] container &Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-228fn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod coredns-5d78c9869d-wf96w_kube-system(53c4ae71-7ecb-462f-9619-a1376ccd498b): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Jan 28 09:38:07 minikube kubelet[1169]: E0128 09:38:07.783754    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/coredns-5d78c9869d-wf96w" podUID=53c4ae71-7ecb-462f-9619-a1376ccd498b
Jan 28 09:38:18 minikube kubelet[1169]: I0128 09:38:18.781912    1169 scope.go:115] "RemoveContainer" containerID="592cb096c157b327da890c174e58418231056c514977d081b2312b2643b84f75"
Jan 28 09:38:18 minikube kubelet[1169]: I0128 09:38:18.782168    1169 scope.go:115] "RemoveContainer" containerID="e2af9e0129377ed3fccda513368e6b3df7fbcf0f7326554881489a95c20ed0b9"
Jan 28 09:38:19 minikube kubelet[1169]: I0128 09:38:19.782303    1169 scope.go:115] "RemoveContainer" containerID="f279f323a985d052c4f82bd44d20b4089c42c53fd8ed87d539d1612e7e5af8ae"
Jan 28 09:39:00 minikube kubelet[1169]: I0128 09:39:00.944413    1169 scope.go:115] "RemoveContainer" containerID="592cb096c157b327da890c174e58418231056c514977d081b2312b2643b84f75"
Jan 28 09:39:00 minikube kubelet[1169]: I0128 09:39:00.945489    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:00 minikube kubelet[1169]: E0128 09:39:00.947025    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:39:11 minikube kubelet[1169]: I0128 09:39:11.781308    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:11 minikube kubelet[1169]: E0128 09:39:11.782410    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:39:22 minikube kubelet[1169]: I0128 09:39:22.782129    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:22 minikube kubelet[1169]: E0128 09:39:22.782556    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:39:35 minikube kubelet[1169]: I0128 09:39:35.782208    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:35 minikube kubelet[1169]: E0128 09:39:35.783168    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:39:46 minikube kubelet[1169]: I0128 09:39:46.780811    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:46 minikube kubelet[1169]: E0128 09:39:46.781085    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:39:59 minikube kubelet[1169]: I0128 09:39:59.782212    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:39:59 minikube kubelet[1169]: E0128 09:39:59.782911    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:40:12 minikube kubelet[1169]: I0128 09:40:12.782766    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:40:12 minikube kubelet[1169]: E0128 09:40:12.785699    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:40:23 minikube kubelet[1169]: I0128 09:40:23.781130    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:40:23 minikube kubelet[1169]: E0128 09:40:23.781492    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:40:34 minikube kubelet[1169]: I0128 09:40:34.781433    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:40:34 minikube kubelet[1169]: E0128 09:40:34.781870    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:40:45 minikube kubelet[1169]: I0128 09:40:45.782567    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:40:45 minikube kubelet[1169]: E0128 09:40:45.783626    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:40:57 minikube kubelet[1169]: I0128 09:40:57.783191    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:40:57 minikube kubelet[1169]: E0128 09:40:57.784127    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:41:10 minikube kubelet[1169]: I0128 09:41:10.782245    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:41:10 minikube kubelet[1169]: E0128 09:41:10.782956    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:41:24 minikube kubelet[1169]: I0128 09:41:24.781680    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:41:24 minikube kubelet[1169]: E0128 09:41:24.782369    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:41:37 minikube kubelet[1169]: I0128 09:41:37.781886    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:41:37 minikube kubelet[1169]: E0128 09:41:37.782677    1169 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(09de5948-0fc6-4f30-8413-0395a28f1b39)\"" pod="kube-system/storage-provisioner" podUID=09de5948-0fc6-4f30-8413-0395a28f1b39
Jan 28 09:41:51 minikube kubelet[1169]: I0128 09:41:51.782226    1169 scope.go:115] "RemoveContainer" containerID="f3d2b800613bea05e91ce82ce84a0db3d2c037ccb2f47a3fe07c0d870664c58b"
Jan 28 09:46:14 minikube kubelet[1169]: I0128 09:46:14.855697    1169 topology_manager.go:212] "Topology Admit Handler"
Jan 28 09:46:14 minikube kubelet[1169]: I0128 09:46:14.922803    1169 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zjg4d\" (UniqueName: \"kubernetes.io/projected/b619dc38-a6cb-419a-a3e3-2b9a73d1b71c-kube-api-access-zjg4d\") pod \"mysql-6dd4c8b9c4-pg8h9\" (UID: \"b619dc38-a6cb-419a-a3e3-2b9a73d1b71c\") " pod="default/mysql-6dd4c8b9c4-pg8h9"
Jan 28 09:46:14 minikube kubelet[1169]: I0128 09:46:14.922893    1169 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"mysql-initdb\" (UniqueName: \"kubernetes.io/configmap/b619dc38-a6cb-419a-a3e3-2b9a73d1b71c-mysql-initdb\") pod \"mysql-6dd4c8b9c4-pg8h9\" (UID: \"b619dc38-a6cb-419a-a3e3-2b9a73d1b71c\") " pod="default/mysql-6dd4c8b9c4-pg8h9"
Jan 28 09:46:18 minikube kubelet[1169]: I0128 09:46:18.077779    1169 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="870cacb2e0e6434446594ed1800327c7125e47d0b1925002fede72fc39bf614a"
Jan 28 09:46:23 minikube kubelet[1169]: I0128 09:46:23.757369    1169 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mysql-6dd4c8b9c4-pg8h9" podStartSLOduration=7.160304344 podCreationTimestamp="2024-01-28 09:46:13 +0000 UTC" firstStartedPulling="2024-01-28 09:46:18.321682162 +0000 UTC m=+554.713197543" lastFinishedPulling="2024-01-28 09:46:21.616847581 +0000 UTC m=+558.008363025" observedRunningTime="2024-01-28 09:46:23.454190086 +0000 UTC m=+559.845705605" watchObservedRunningTime="2024-01-28 09:46:23.455469826 +0000 UTC m=+559.846985361"
Jan 28 09:47:26 minikube kubelet[1169]: I0128 09:47:26.480166    1169 topology_manager.go:212] "Topology Admit Handler"
Jan 28 09:47:26 minikube kubelet[1169]: I0128 09:47:26.564908    1169 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sbzrk\" (UniqueName: \"kubernetes.io/projected/28072a28-d907-41dd-8e09-55f174531762-kube-api-access-sbzrk\") pod \"two-tier-app-5467879f76-wsm25\" (UID: \"28072a28-d907-41dd-8e09-55f174531762\") " pod="default/two-tier-app-5467879f76-wsm25"
Jan 28 09:47:29 minikube kubelet[1169]: I0128 09:47:29.624315    1169 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9dc9f85f8a728ac39bc11bb099852595930ca7a4e82f873ce4cc1b90a0f69993"

* 
* ==> storage-provisioner [cd7c656fd96d] <==
* I0128 09:41:53.653824       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0128 09:41:53.795781       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0128 09:41:53.804848       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0128 09:42:11.291257       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0128 09:42:11.291509       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"afd48cf6-d8b9-4140-a851-9cbcc8c4ea23", APIVersion:"v1", ResourceVersion:"3612", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f5668c52-d7dd-4d42-a833-1e65ad7ee524 became leader
I0128 09:42:11.292023       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f5668c52-d7dd-4d42-a833-1e65ad7ee524!
I0128 09:42:11.394632       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f5668c52-d7dd-4d42-a833-1e65ad7ee524!

* 
* ==> storage-provisioner [f3d2b800613b] <==
* I0128 09:38:29.227367       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0128 09:38:59.445634       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

